"""
PDF ä»£ç é“¾æ¥æå–å™¨ - ä»è®ºæ–‡ PDF ä¸­æå–ä»£ç ä»“åº“é“¾æ¥
"""

import os
import re
from typing import List, Dict, Optional, Tuple
import pymupdf  # PyMuPDF
import requests
from bs4 import BeautifulSoup

from config import CODE_HOST_DOMAINS, CODE_KEYWORDS
from utils import is_valid_repo_url, normalize_url
from llm_client import llm_client


def extract_text_from_pdf(pdf_path: str) -> Optional[str]:
    """
    ä» PDF ä¸­æå–å…¨æ–‡æœ¬
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        
    Returns:
        æå–çš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å› None
    """
    if not os.path.exists(pdf_path):
        print(f"    âš ï¸  PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return None
    
    try:
        doc = pymupdf.open(pdf_path)
        text = ""
        
        # åªæå–å‰å‡ é¡µï¼ˆé€šå¸¸ä»£ç é“¾æ¥åœ¨å‰é¢ï¼‰
        max_pages = min(5, len(doc))
        
        for page_num in range(max_pages):
            page = doc[page_num]
            text += page.get_text()
        
        doc.close()
        return text
        
    except Exception as e:
        print(f"    âŒ PDF æå–å¤±è´¥: {e}")
        return None


def extract_url_patterns_with_context(text: str, context_chars: int = 50) -> List[Dict]:
    """
    ä»æ–‡æœ¬ä¸­æå– URL æ¨¡å¼åŠå…¶ä¸Šä¸‹æ–‡
    
    Args:
        text: æ–‡æœ¬å†…å®¹
        context_chars: ä¸Šä¸‹æ–‡å­—ç¬¦æ•°
        
    Returns:
        åŒ…å« URL æ¨¡å¼å’Œä¸Šä¸‹æ–‡çš„å­—å…¸åˆ—è¡¨
    """
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ URL èµ·å§‹ä½ç½® (http:// æˆ– https://)
    url_start_pattern = r'https?://'
    url_patterns = []
    
    for match in re.finditer(url_start_pattern, text, re.IGNORECASE):
        start_pos = match.start()
        url_start = match.group()
        
        # æå– URL èµ·å§‹ä½ç½®ä¹‹å‰å’Œä¹‹åçš„å†…å®¹
        context_start = max(0, start_pos - context_chars)
        # ç²—ç•¥ä¼°è®¡ URL å¯èƒ½çš„ç»“æŸä½ç½®ï¼ˆæœ€å¤š200ä¸ªå­—ç¬¦ï¼‰
        url_end_estimate = min(len(text), start_pos + 200)
        context_end = min(len(text), url_end_estimate + context_chars)
        
        # æå–å®Œæ•´ä¸Šä¸‹æ–‡
        full_context = text[context_start:context_end]
        
        # æå– URL èµ·å§‹åçš„å†…å®¹ï¼ˆç”¨äº LLM åˆ†æï¼‰
        url_candidate = text[start_pos:url_end_estimate]
        
        url_patterns.append({
            "url_start": url_start,
            "position": start_pos,
            "before_context": text[context_start:start_pos],
            "url_candidate": url_candidate,
            "after_context": text[url_end_estimate:context_end],
            "full_context": full_context
        })
    
    return url_patterns


def extract_urls_with_llm(url_patterns: List[Dict]) -> List[str]:
    """
    ä½¿ç”¨ LLM ä» URL æ¨¡å¼å’Œä¸Šä¸‹æ–‡ä¸­ç²¾ç¡®è¯†åˆ«çœŸæ­£çš„ URL
    
    Args:
        url_patterns: URL æ¨¡å¼å’Œä¸Šä¸‹æ–‡åˆ—è¡¨
        
    Returns:
        ç²¾ç¡®çš„ URL åˆ—è¡¨
    """
    if not url_patterns:
        return []
    
    print(f"    ğŸ¤– ä½¿ç”¨ LLM ä» {len(url_patterns)} ä¸ªå€™é€‰ä¸­ç²¾ç¡®æå– URL...")
    
    # æ„é€ è¾“å…¥
    patterns_text = []
    for i, pattern in enumerate(url_patterns, 1):
        patterns_text.append(
            f"{i}. ä¸Šä¸‹æ–‡:\n"
            f"   å‰: ...{pattern['before_context'][-30:]}\n"
            f"   URLå€™é€‰: {pattern['url_start']}[è¿™é‡Œæ˜¯URL]\n"
            f"   å: {pattern['after_context'][:30]}...\n"
            f"   å®Œæ•´å€™é€‰: {pattern['url_candidate'][:100]}..."
        )
    
    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ª URL æå–ä¸“å®¶ã€‚ä» PDF æ–‡æœ¬ä¸­ç²¾ç¡®è¯†åˆ« GitHub/GitLab ç­‰ä»£ç ä»“åº“çš„å®Œæ•´ URLã€‚æ³¨æ„å¤„ç†æ¢è¡Œã€å¤šä½™å­—ç¬¦ç­‰é—®é¢˜ã€‚"
        },
        {
            "role": "user",
            "content": f"""ä»ä»¥ä¸‹ PDF æ–‡æœ¬ç‰‡æ®µä¸­æå–å®Œæ•´çš„ä»£ç ä»“åº“ URLï¼ˆGitHubã€GitLabç­‰ï¼‰ã€‚

{chr(10).join(patterns_text)}

è¦æ±‚ï¼š
1. æå–å®Œæ•´çš„ URLï¼ˆåŒ…æ‹¬åè®®ã€åŸŸåã€è·¯å¾„ï¼‰
2. å¤„ç† PDF ä¸­çš„æ¢è¡Œé—®é¢˜ï¼ˆURL å¯èƒ½è¢«æ–­æˆå¤šè¡Œï¼‰
3. ç§»é™¤ URL åé¢æ— å…³çš„å†…å®¹ï¼ˆå¦‚è®ºæ–‡æ ‡é¢˜ã€é¡µç ç­‰ï¼‰
4. æ ‡å‡†çš„ä»“åº“ URL æ ¼å¼ï¼šhttps://github.com/ç”¨æˆ·å/ä»“åº“å
5. å¦‚æœä¸æ˜¯ä»£ç ä»“åº“ URLï¼Œè¿”å› null

ç”¨ JSON æ ¼å¼å›å¤ï¼š
{{
  "urls": [
    {{
      "index": 1,
      "url": "æå–çš„å®Œæ•´URLæˆ–null",
      "reason": "æå–ç†ç”±ï¼ˆä¸­æ–‡ï¼‰"
    }}
  ]
}}"""
        }
    ]
    
    response = llm_client.call_json(messages, temperature=0.1)
    
    if not response or "urls" not in response:
        print(f"    âš ï¸  LLM æå–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™æå–
        return extract_urls_fallback([p["url_candidate"] for p in url_patterns])
    
    extracted_urls = []
    for item in response["urls"]:
        url = item.get("url")
        reason = item.get("reason", "")
        
        if url and url != "null" and url.lower() != "null":
            extracted_urls.append(url)
            print(f"    âœ“ æå–: {url}")
            if reason:
                print(f"      ç†ç”±: {reason}")
    
    return extracted_urls


def extract_urls_fallback(candidates: List[str]) -> List[str]:
    """
    å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– URL
    
    Args:
        candidates: URL å€™é€‰åˆ—è¡¨
        
    Returns:
        URL åˆ—è¡¨
    """
    urls = []
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    
    for candidate in candidates:
        # å¤„ç†æ¢è¡Œ
        candidate = re.sub(r'\n\s*', '', candidate)
        
        matches = re.findall(url_pattern, candidate)
        for url in matches:
            # æ¸…ç†
            url = url.rstrip('.,;:!?)\']Â»')
            url = re.sub(r'\.(\d+)[A-Z][a-zA-Z]+.*$', r'.\1', url)
            
            if len(url) > 15 and 'github.com' in url.lower() or 'gitlab.com' in url.lower():
                urls.append(url)
    
    return list(set(urls))
    """
    ä»é¡¹ç›®ä¸»é¡µä¸­æå– GitHub é“¾æ¥
    
    Args:
        url: é¡¹ç›®ä¸»é¡µ URL
        
    Returns:
        GitHub é“¾æ¥ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å› None
    """
    try:
        print(f"      ğŸ”— è®¿é—®é¡¹ç›®é¡µé¢: {url}")
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ GitHub é“¾æ¥
            if 'github.com' in href.lower():
                # æ ‡å‡†åŒ– URL
                if href.startswith('//'):
                    href = 'https:' + href
                elif href.startswith('/'):
                    continue
                
                # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ä»“åº“é“¾æ¥
                if is_valid_repo_url(href, ['github.com']):
                    print(f"      âœ… æ‰¾åˆ° GitHub é“¾æ¥: {href}")
                    return normalize_url(href)
        
        return None
        
    except Exception as e:
        print(f"      âš ï¸  è®¿é—®é¡¹ç›®é¡µé¢å¤±è´¥: {e}")
        return None


def clean_urls_with_llm(urls: List[str], paper_title: str) -> List[str]:
    """
    ä½¿ç”¨ LLM æ¸…ç†å’ŒéªŒè¯æå–çš„ URLï¼Œç§»é™¤é”™è¯¯æ‹¼æ¥çš„éƒ¨åˆ†
    
    Args:
        urls: åŸå§‹ URL åˆ—è¡¨
        paper_title: è®ºæ–‡æ ‡é¢˜ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
        
    Returns:
        æ¸…ç†åçš„ URL åˆ—è¡¨
    """
    if not urls:
        return []
    
    # å¦‚æœURLçœ‹èµ·æ¥éƒ½æ­£å¸¸ï¼ˆæ²¡æœ‰å¼‚å¸¸é•¿çš„è·¯å¾„ï¼‰ï¼Œç›´æ¥è¿”å›
    suspicious = False
    for url in urls:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸ç‰¹å¾
        path = url.split('github.com/')[-1] if 'github.com' in url else ''
        if len(path) > 50 or any(char.isupper() and i > 0 and path[i-1].islower() for i, char in enumerate(path)):
            suspicious = True
            break
    
    if not suspicious:
        return urls
    
    print(f"    ğŸ¤– ä½¿ç”¨ LLM æ¸…ç†å¯ç–‘ URL...")
    
    urls_text = "\n".join([f"{i+1}. {url}" for i, url in enumerate(urls)])
    
    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªURLæ¸…ç†ä¸“å®¶ã€‚å¸®åŠ©è¯†åˆ«å’Œä¿®æ­£ä»PDFä¸­æå–çš„GitHub URLï¼Œç§»é™¤é”™è¯¯æ‹¼æ¥çš„å†…å®¹ï¼ˆå¦‚è®ºæ–‡æ ‡é¢˜ç­‰ï¼‰ã€‚"
        },
        {
            "role": "user",
            "content": f"""è®ºæ–‡æ ‡é¢˜: {paper_title}

ä»PDFä¸­æå–åˆ°ä»¥ä¸‹URLï¼Œä½†å¯èƒ½åŒ…å«é”™è¯¯æ‹¼æ¥çš„å†…å®¹ï¼ˆå¦‚æŠŠä¸‹ä¸€ç¯‡è®ºæ–‡æ ‡é¢˜ä¹Ÿæ‹¼æ¥è¿›å»äº†ï¼‰ï¼š
{urls_text}

è¯·åˆ†ææ¯ä¸ªURLï¼Œç§»é™¤ä¸å±äºURLçš„éƒ¨åˆ†ï¼Œè¿”å›æ¸…ç†åçš„æ­£ç¡®URLã€‚

è¦æ±‚ï¼š
1. GitHub URL æ ¼å¼é€šå¸¸æ˜¯: https://github.com/ç”¨æˆ·å/ä»“åº“å
2. ç§»é™¤URLåé¢é”™è¯¯æ‹¼æ¥çš„è®ºæ–‡æ ‡é¢˜ã€é¡µç ç­‰å†…å®¹
3. å¦‚æœURLæ— æ³•ä¿®æ­£ï¼Œè¿”å›null

ç”¨JSONæ ¼å¼å›å¤ï¼š
{{
  "cleaned_urls": [
    {{
      "original": "åŸå§‹URL",
      "cleaned": "æ¸…ç†åçš„URLæˆ–null",
      "reason": "æ¸…ç†åŸå› ï¼ˆä¸­æ–‡ï¼‰"
    }}
  ]
}}"""
        }
    ]
    
    response = llm_client.call_json(messages, temperature=0.1)
    
    if not response or "cleaned_urls" not in response:
        print(f"    âš ï¸  LLM æ¸…ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹URL")
        return urls
    
    cleaned = []
    for item in response["cleaned_urls"]:
        cleaned_url = item.get("cleaned")
        reason = item.get("reason", "")
        
        if cleaned_url and cleaned_url != "null":
            cleaned.append(cleaned_url)
            if cleaned_url != item.get("original"):
                print(f"    âœ‚ï¸  ä¿®æ­£: {item.get('original')} -> {cleaned_url}")
                print(f"       åŸå› : {reason}")
    
    return cleaned if cleaned else urls


def filter_code_urls(urls: List[str]) -> List[str]:
    """
    è¿‡æ»¤å‡ºä»£ç ä»“åº“ URLï¼Œå¹¶ä»é¡¹ç›®ä¸»é¡µæå– GitHub é“¾æ¥
    
    Args:
        urls: URL åˆ—è¡¨
        
    Returns:
        ä»£ç ä»“åº“ URL åˆ—è¡¨
    """
    code_urls = []
    project_pages = []  # å¯èƒ½æ˜¯é¡¹ç›®ä¸»é¡µçš„ URL
    
    for url in urls:
        # ç›´æ¥æ˜¯ä»£ç ä»“åº“
        if is_valid_repo_url(url, CODE_HOST_DOMAINS):
            normalized = normalize_url(url)
            if normalized not in code_urls:
                code_urls.append(normalized)
        
        # å¯èƒ½æ˜¯é¡¹ç›®ä¸»é¡µï¼ˆå¸¸è§æ¨¡å¼ï¼‰
        elif any(pattern in url.lower() for pattern in [
            '.github.io',
            'github.io',
            'project',
            'page',
            'demo',
            'site'
        ]):
            # é¿å…æ˜æ˜¾ä¸æ˜¯é¡¹ç›®é¡µçš„é“¾æ¥
            if not any(skip in url.lower() for skip in ['arxiv.org', 'doi.org', 'youtube.com']):
                project_pages.append(url)
    
    # å¦‚æœæ²¡æœ‰ç›´æ¥æ‰¾åˆ°ä»£ç é“¾æ¥ï¼Œå°è¯•ä»é¡¹ç›®é¡µæå–
    if not code_urls and project_pages:
        print(f"    ğŸ” æœªæ‰¾åˆ°ç›´æ¥ä»£ç é“¾æ¥ï¼Œå°è¯•ä» {len(project_pages)} ä¸ªé¡¹ç›®é¡µé¢æå–...")
        
        for page_url in project_pages[:3]:  # æœ€å¤šå°è¯•3ä¸ª
            github_url = extract_github_from_project_page(page_url)
            if github_url and github_url not in code_urls:
                code_urls.append(github_url)
    
    return code_urls


def find_urls_with_context(text: str, urls: List[str]) -> List[Tuple[str, str]]:
    """
    æ‰¾åˆ° URL åŠå…¶ä¸Šä¸‹æ–‡ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦æ˜¯ä»£ç é“¾æ¥ï¼‰
    
    Args:
        text: æ–‡æœ¬å†…å®¹
        urls: URL åˆ—è¡¨
        
    Returns:
        (url, context) å…ƒç»„åˆ—è¡¨
    """
    results = []
    
    for url in urls:
        # æ‰¾åˆ° URL åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
        idx = text.find(url)
        if idx == -1:
            continue
        
        # æå–ä¸Šä¸‹æ–‡ï¼ˆå‰åå„100ä¸ªå­—ç¬¦ï¼‰
        start = max(0, idx - 100)
        end = min(len(text), idx + len(url) + 100)
        context = text[start:end]
        
        results.append((url, context))
    
    return results


def is_likely_code_url(context: str) -> bool:
    """
    æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ URL æ˜¯å¦å¯èƒ½æ˜¯ä»£ç é“¾æ¥
    
    Args:
        context: URL çš„ä¸Šä¸‹æ–‡
        
    Returns:
        æ˜¯å¦å¯èƒ½æ˜¯ä»£ç é“¾æ¥
    """
    context_lower = context.lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»£ç ç›¸å…³å…³é”®è¯
    for keyword in CODE_KEYWORDS:
        if keyword in context_lower:
            return True
    
    return False


def select_official_repo_with_llm(
    paper_data: Dict,
    candidate_urls: List[str]
) -> Optional[str]:
    """
    ä½¿ç”¨ LLM ä»å€™é€‰ URL ä¸­é€‰æ‹©æœ€å¯èƒ½çš„å®˜æ–¹å®ç°
    
    Args:
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        candidate_urls: å€™é€‰ URL åˆ—è¡¨
        
    Returns:
        é€‰ä¸­çš„ URLï¼Œå¦‚æœéƒ½ä¸æ˜¯è¿”å› None
    """
    if not candidate_urls:
        return None
    
    if len(candidate_urls) == 1:
        return candidate_urls[0]
    
    # æ„é€  prompt
    urls_text = "\n".join([f"{i+1}. {url}" for i, url in enumerate(candidate_urls)])
    
    messages = [
        {
            "role": "system",
            "content": "You are a tool that picks the most likely official code repository for a computer vision paper. Reply in JSON format."
        },
        {
            "role": "user",
            "content": f"""Paper title: "{paper_data.get('title', '')}"
Venue: {paper_data.get('conference', '')} {paper_data.get('year', '')}
Abstract: "{paper_data.get('abstract', '')[:500]}..."

Found URLs in PDF:
{urls_text}

From these URLs, which one is MOST likely the official implementation of the paper?
Reply in JSON format:
{{
  "selected_url": "<url or null>",
  "reason": "brief explanation"
}}"""
        }
    ]
    
    response = llm_client.call_json(messages)
    
    if response and "selected_url" in response:
        selected_url = response["selected_url"]
        if selected_url and selected_url.lower() != "null":
            print(f"    ğŸ¤– LLM é€‰æ‹©: {selected_url}")
            print(f"       ç†ç”±: {response.get('reason', 'N/A')}")
            return normalize_url(selected_url)
    
    return None


def extract_code_urls_from_pdf(
    pdf_path: str,
    paper_data: Dict,
    use_llm: bool = True
) -> Dict[str, any]:
    """
    ä» PDF ä¸­æå–ä»£ç ä»“åº“ URL
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM æ¥é€‰æ‹©å€™é€‰é“¾æ¥
        
    Returns:
        æå–ç»“æœå­—å…¸
    """
    print(f"  ğŸ“„ ä» PDF æå–ä»£ç é“¾æ¥...")
    
    # 1. æå–æ–‡æœ¬
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return {
            "success": False,
            "official_repo_url": None,
            "candidates": [],
            "source": "pdf"
        }
    
    # 2. æå– URL æ¨¡å¼å’Œä¸Šä¸‹æ–‡
    url_patterns = extract_url_patterns_with_context(text, context_chars=50)
    print(f"    æ‰¾åˆ° {len(url_patterns)} ä¸ª URL æ¨¡å¼")
    
    if not url_patterns:
        return {
            "success": False,
            "official_repo_url": None,
            "candidates": [],
            "source": "pdf"
        }
    
    # 3. ä½¿ç”¨ LLM ç²¾ç¡®æå– URL
    if use_llm:
        all_urls = extract_urls_with_llm(url_patterns)
    else:
        all_urls = extract_urls_fallback([p["url_candidate"] for p in url_patterns])
    
    print(f"    æå–å‡º {len(all_urls)} ä¸ª URL")
    
    # 4. è¿‡æ»¤å‡ºä»£ç ä»“åº“ URL
    code_urls = filter_code_urls(all_urls)
    print(f"    å…¶ä¸­ {len(code_urls)} ä¸ªæ˜¯ä»£ç ä»“åº“ URL")
    
    if not code_urls:
        return {
            "success": False,
            "official_repo_url": None,
            "candidates": [],
            "source": "pdf"
        }
    
    # 5. æ ¹æ®ä¸Šä¸‹æ–‡è¿‡æ»¤
    urls_with_context = find_urls_with_context(text, code_urls)
    likely_code_urls = [
        url for url, context in urls_with_context
        if is_likely_code_url(context)
    ]
    
    # å¦‚æœæœ‰æ˜ç¡®çš„ä»£ç ç›¸å…³ URLï¼Œä¼˜å…ˆä½¿ç”¨
    candidates = likely_code_urls if likely_code_urls else code_urls
    print(f"    ç»ä¸Šä¸‹æ–‡åˆ†æï¼Œ{len(candidates)} ä¸ªå€™é€‰é“¾æ¥")
    
    # 6. é€‰æ‹©å®˜æ–¹ä»“åº“
    official_url = None
    
    if len(candidates) == 1:
        official_url = candidates[0]
        print(f"    âœ… æ‰¾åˆ°å”¯ä¸€å€™é€‰: {official_url}")
    elif len(candidates) > 1 and use_llm:
        official_url = select_official_repo_with_llm(paper_data, candidates)
    elif len(candidates) > 1:
        # ä¸ä½¿ç”¨ LLMï¼Œé€‰ç¬¬ä¸€ä¸ª
        official_url = candidates[0]
        print(f"    âš ï¸  æœ‰å¤šä¸ªå€™é€‰ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª: {official_url}")
    
    return {
        "success": bool(official_url),
        "official_repo_url": official_url,
        "candidates": candidates,
        "source": "pdf"
    }


def process_paper_pdf(paper_dir: str, paper_data: Dict, use_llm: bool = True) -> Dict:
    """
    å¤„ç†å•ç¯‡è®ºæ–‡çš„ PDF æå–
    
    Args:
        paper_dir: è®ºæ–‡ç›®å½•
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM
        
    Returns:
        æå–ç»“æœ
    """
    pdf_path = os.path.join(paper_dir, "paper.pdf")
    
    if not os.path.exists(pdf_path):
        print(f"    âš ï¸  PDF æ–‡ä»¶ä¸å­˜åœ¨")
        return {
            "success": False,
            "official_repo_url": None,
            "candidates": [],
            "source": "pdf"
        }
    
    return extract_code_urls_from_pdf(pdf_path, paper_data, use_llm)


if __name__ == "__main__":
    # æµ‹è¯•
    from config import PAPERS_ROOT_DIR
    from utils import get_all_paper_dirs, load_paper_data
    
    print("æµ‹è¯• PDF æå–å™¨...")
    paper_dirs = get_all_paper_dirs(PAPERS_ROOT_DIR)
    
    if paper_dirs:
        test_dir = paper_dirs[0]
        print(f"\næµ‹è¯•è®ºæ–‡: {os.path.basename(test_dir)}")
        
        paper_data = load_paper_data(test_dir)
        if paper_data:
            result = process_paper_pdf(test_dir, paper_data, use_llm=True)
            print(f"\nç»“æœ: {result}")
