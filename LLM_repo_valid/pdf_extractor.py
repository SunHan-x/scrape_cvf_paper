"""
PDF ä»£ç é“¾æ¥æå–å™¨ - ä»è®ºæ–‡ PDF ä¸­æå–ä»£ç ä»“åº“é“¾æ¥
"""

import os
import re
from typing import List, Dict, Optional, Tuple
import pymupdf  # PyMuPDF

from config import CODE_HOST_DOMAINS, CODE_KEYWORDS
from utils import is_valid_repo_url, normalize_url
from llm_client import llm_client


def extract_text_from_pdf(pdf_path: str) -> Optional[str]:
    """
    ä» PDF ä¸­æå–å…¨æ–‡æœ¬
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        
    Returns:
        æå–çš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å› None
    """
    if not os.path.exists(pdf_path):
        print(f"    âš ï¸  PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return None
    
    try:
        doc = pymupdf.open(pdf_path)
        text = ""
        
        # åªæå–å‰å‡ é¡µï¼ˆé€šå¸¸ä»£ç é“¾æ¥åœ¨å‰é¢ï¼‰
        max_pages = min(5, len(doc))
        
        for page_num in range(max_pages):
            page = doc[page_num]
            text += page.get_text()
        
        doc.close()
        return text
        
    except Exception as e:
        print(f"    âŒ PDF æå–å¤±è´¥: {e}")
        return None


def extract_urls_from_text(text: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰ URL
    
    Args:
        text: æ–‡æœ¬å†…å®¹
        
    Returns:
        URL åˆ—è¡¨
    """
    # URL æ­£åˆ™è¡¨è¾¾å¼
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    
    urls = re.findall(url_pattern, text, re.IGNORECASE)
    
    # æ¸…ç† URLï¼ˆç§»é™¤å°¾éƒ¨æ ‡ç‚¹ç¬¦å·ï¼‰
    cleaned_urls = []
    for url in urls:
        url = url.rstrip('.,;:!?)')
        cleaned_urls.append(url)
    
    return list(set(cleaned_urls))  # å»é‡


def filter_code_urls(urls: List[str]) -> List[str]:
    """
    è¿‡æ»¤å‡ºä»£ç ä»“åº“ URL
    
    Args:
        urls: URL åˆ—è¡¨
        
    Returns:
        ä»£ç ä»“åº“ URL åˆ—è¡¨
    """
    code_urls = []
    
    for url in urls:
        if is_valid_repo_url(url, CODE_HOST_DOMAINS):
            normalized = normalize_url(url)
            if normalized not in code_urls:
                code_urls.append(normalized)
    
    return code_urls


def find_urls_with_context(text: str, urls: List[str]) -> List[Tuple[str, str]]:
    """
    æ‰¾åˆ° URL åŠå…¶ä¸Šä¸‹æ–‡ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦æ˜¯ä»£ç é“¾æ¥ï¼‰
    
    Args:
        text: æ–‡æœ¬å†…å®¹
        urls: URL åˆ—è¡¨
        
    Returns:
        (url, context) å…ƒç»„åˆ—è¡¨
    """
    results = []
    
    for url in urls:
        # æ‰¾åˆ° URL åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
        idx = text.find(url)
        if idx == -1:
            continue
        
        # æå–ä¸Šä¸‹æ–‡ï¼ˆå‰åå„100ä¸ªå­—ç¬¦ï¼‰
        start = max(0, idx - 100)
        end = min(len(text), idx + len(url) + 100)
        context = text[start:end]
        
        results.append((url, context))
    
    return results


def is_likely_code_url(context: str) -> bool:
    """
    æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ URL æ˜¯å¦å¯èƒ½æ˜¯ä»£ç é“¾æ¥
    
    Args:
        context: URL çš„ä¸Šä¸‹æ–‡
        
    Returns:
        æ˜¯å¦å¯èƒ½æ˜¯ä»£ç é“¾æ¥
    """
    context_lower = context.lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»£ç ç›¸å…³å…³é”®è¯
    for keyword in CODE_KEYWORDS:
        if keyword in context_lower:
            return True
    
    return False


def select_official_repo_with_llm(
    paper_data: Dict,
    candidate_urls: List[str]
) -> Optional[str]:
    """
    ä½¿ç”¨ LLM ä»å€™é€‰ URL ä¸­é€‰æ‹©æœ€å¯èƒ½çš„å®˜æ–¹å®ç°
    
    Args:
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        candidate_urls: å€™é€‰ URL åˆ—è¡¨
        
    Returns:
        é€‰ä¸­çš„ URLï¼Œå¦‚æœéƒ½ä¸æ˜¯è¿”å› None
    """
    if not candidate_urls:
        return None
    
    if len(candidate_urls) == 1:
        return candidate_urls[0]
    
    # æ„é€  prompt
    urls_text = "\n".join([f"{i+1}. {url}" for i, url in enumerate(candidate_urls)])
    
    messages = [
        {
            "role": "system",
            "content": "You are a tool that picks the most likely official code repository for a computer vision paper. Reply in JSON format."
        },
        {
            "role": "user",
            "content": f"""Paper title: "{paper_data.get('title', '')}"
Venue: {paper_data.get('conference', '')} {paper_data.get('year', '')}
Abstract: "{paper_data.get('abstract', '')[:500]}..."

Found URLs in PDF:
{urls_text}

From these URLs, which one is MOST likely the official implementation of the paper?
Reply in JSON format:
{{
  "selected_url": "<url or null>",
  "reason": "brief explanation"
}}"""
        }
    ]
    
    response = llm_client.call_json(messages)
    
    if response and "selected_url" in response:
        selected_url = response["selected_url"]
        if selected_url and selected_url.lower() != "null":
            print(f"    ğŸ¤– LLM é€‰æ‹©: {selected_url}")
            print(f"       ç†ç”±: {response.get('reason', 'N/A')}")
            return normalize_url(selected_url)
    
    return None


def extract_code_urls_from_pdf(
    pdf_path: str,
    paper_data: Dict,
    use_llm: bool = True
) -> Dict[str, any]:
    """
    ä» PDF ä¸­æå–ä»£ç ä»“åº“ URL
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM æ¥é€‰æ‹©å€™é€‰é“¾æ¥
        
    Returns:
        æå–ç»“æœå­—å…¸
    """
    print(f"  ğŸ“„ ä» PDF æå–ä»£ç é“¾æ¥...")
    
    # 1. æå–æ–‡æœ¬
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return {
            "success": False,
            "official_repo_url": None,
            "candidates": [],
            "source": "pdf"
        }
    
    # 2. æå–æ‰€æœ‰ URL
    all_urls = extract_urls_from_text(text)
    print(f"    æ‰¾åˆ° {len(all_urls)} ä¸ª URL")
    
    # 3. è¿‡æ»¤å‡ºä»£ç ä»“åº“ URL
    code_urls = filter_code_urls(all_urls)
    print(f"    å…¶ä¸­ {len(code_urls)} ä¸ªæ˜¯ä»£ç ä»“åº“ URL")
    
    if not code_urls:
        return {
            "success": False,
            "official_repo_url": None,
            "candidates": [],
            "source": "pdf"
        }
    
    # 4. æ ¹æ®ä¸Šä¸‹æ–‡è¿‡æ»¤
    urls_with_context = find_urls_with_context(text, code_urls)
    likely_code_urls = [
        url for url, context in urls_with_context
        if is_likely_code_url(context)
    ]
    
    # å¦‚æœæœ‰æ˜ç¡®çš„ä»£ç ç›¸å…³ URLï¼Œä¼˜å…ˆä½¿ç”¨
    candidates = likely_code_urls if likely_code_urls else code_urls
    print(f"    ç»ä¸Šä¸‹æ–‡åˆ†æï¼Œ{len(candidates)} ä¸ªå€™é€‰é“¾æ¥")
    
    # 5. é€‰æ‹©å®˜æ–¹ä»“åº“
    official_url = None
    
    if len(candidates) == 1:
        official_url = candidates[0]
        print(f"    âœ… æ‰¾åˆ°å”¯ä¸€å€™é€‰: {official_url}")
    elif len(candidates) > 1 and use_llm:
        official_url = select_official_repo_with_llm(paper_data, candidates)
    elif len(candidates) > 1:
        # ä¸ä½¿ç”¨ LLMï¼Œé€‰ç¬¬ä¸€ä¸ª
        official_url = candidates[0]
        print(f"    âš ï¸  æœ‰å¤šä¸ªå€™é€‰ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª: {official_url}")
    
    return {
        "success": bool(official_url),
        "official_repo_url": official_url,
        "candidates": candidates,
        "source": "pdf"
    }


def process_paper_pdf(paper_dir: str, paper_data: Dict, use_llm: bool = True) -> Dict:
    """
    å¤„ç†å•ç¯‡è®ºæ–‡çš„ PDF æå–
    
    Args:
        paper_dir: è®ºæ–‡ç›®å½•
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM
        
    Returns:
        æå–ç»“æœ
    """
    pdf_path = os.path.join(paper_dir, "paper.pdf")
    
    if not os.path.exists(pdf_path):
        print(f"    âš ï¸  PDF æ–‡ä»¶ä¸å­˜åœ¨")
        return {
            "success": False,
            "official_repo_url": None,
            "candidates": [],
            "source": "pdf"
        }
    
    return extract_code_urls_from_pdf(pdf_path, paper_data, use_llm)


if __name__ == "__main__":
    # æµ‹è¯•
    from config import PAPERS_ROOT_DIR
    from utils import get_all_paper_dirs, load_paper_data
    
    print("æµ‹è¯• PDF æå–å™¨...")
    paper_dirs = get_all_paper_dirs(PAPERS_ROOT_DIR)
    
    if paper_dirs:
        test_dir = paper_dirs[0]
        print(f"\næµ‹è¯•è®ºæ–‡: {os.path.basename(test_dir)}")
        
        paper_data = load_paper_data(test_dir)
        if paper_data:
            result = process_paper_pdf(test_dir, paper_data, use_llm=True)
            print(f"\nç»“æœ: {result}")
