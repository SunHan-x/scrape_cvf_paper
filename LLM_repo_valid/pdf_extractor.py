"""
PDF ä»£ç é“¾æ¥æå–å™¨ - ä»è®ºæ–‡ PDF ä¸­æå–ä»£ç ä»“åº“é“¾æ¥
"""

import os
import re
from typing import List, Dict, Optional, Tuple
import pymupdf  # PyMuPDF
import requests
from bs4 import BeautifulSoup

from config import CODE_HOST_DOMAINS, CODE_KEYWORDS
from utils import is_valid_repo_url, normalize_url
from llm_client import llm_client


def extract_text_from_pdf(pdf_path: str) -> Optional[str]:
    """
    ä» PDF ä¸­æå–å…¨æ–‡æœ¬
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        
    Returns:
        æå–çš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å› None
    """
    if not os.path.exists(pdf_path):
        print(f"    âš ï¸  PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return None
    
    try:
        doc = pymupdf.open(pdf_path)
        text = ""
        
        # åªæå–å‰å‡ é¡µï¼ˆé€šå¸¸ä»£ç é“¾æ¥åœ¨å‰é¢ï¼‰
        max_pages = min(5, len(doc))
        
        for page_num in range(max_pages):
            page = doc[page_num]
            text += page.get_text()
        
        doc.close()
        return text
        
    except Exception as e:
        print(f"    âŒ PDF æå–å¤±è´¥: {e}")
        return None


def extract_urls_from_text(text: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰ URLï¼Œå¤„ç†æ¢è¡Œæ–­å¼€çš„ URL
    
    Args:
        text: æ–‡æœ¬å†…å®¹
        
    Returns:
        URL åˆ—è¡¨
    """
    # é¢„å¤„ç†ï¼šå¤„ç†æ¢è¡Œå¯¼è‡´çš„ URL æ–­å¼€
    # å…ˆå¤„ç† https:// æˆ– http:// åé¢ç´§è·Ÿæ¢è¡Œçš„æƒ…å†µ
    text = re.sub(r'(https?://[^\s<>"{}|\\^`\[\]]+?)\n\s*([a-zA-Z0-9\-_/\.]+)', r'\1\2', text)
    
    # URL æ­£åˆ™è¡¨è¾¾å¼ - åŒ¹é… http:// æˆ– https:// å¼€å¤´çš„ URL
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    
    urls = re.findall(url_pattern, text, re.IGNORECASE)
    
    # æ¸…ç† URL
    cleaned_urls = []
    for url in urls:
        # ç§»é™¤å°¾éƒ¨æ ‡ç‚¹å’Œå¸¸è§çš„å¹²æ‰°å­—ç¬¦
        url = url.rstrip('.,;:!?)\']Â»')
        # ç§»é™¤å¯èƒ½çš„æ¢è¡Œç¬¦
        url = url.replace('\n', '').replace('\r', '')
        
        # æ£€æŸ¥ URL æ˜¯å¦åŸºæœ¬æœ‰æ•ˆ
        if len(url) > 10 and '/' in url:
            # ç®€å•éªŒè¯ï¼šè‡³å°‘æœ‰åè®®å’ŒåŸŸåéƒ¨åˆ†
            try:
                # æ£€æŸ¥åŸŸåéƒ¨åˆ†æ˜¯å¦åˆç†
                domain_part = url.split('//')[1].split('/')[0] if '//' in url else ''
                if '.' in domain_part and len(domain_part) > 3:
                    cleaned_urls.append(url)
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä»ç„¶ä¿ç•™ï¼ˆå¯èƒ½æ˜¯ç‰¹æ®Šæ ¼å¼ï¼‰
                cleaned_urls.append(url)
    
    return list(set(cleaned_urls))  # å»é‡


def extract_github_from_project_page(url: str) -> Optional[str]:
    """
    ä»é¡¹ç›®ä¸»é¡µä¸­æå– GitHub é“¾æ¥
    
    Args:
        url: é¡¹ç›®ä¸»é¡µ URL
        
    Returns:
        GitHub é“¾æ¥ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å› None
    """
    try:
        print(f"      ğŸ”— è®¿é—®é¡¹ç›®é¡µé¢: {url}")
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ GitHub é“¾æ¥
            if 'github.com' in href.lower():
                # æ ‡å‡†åŒ– URL
                if href.startswith('//'):
                    href = 'https:' + href
                elif href.startswith('/'):
                    continue
                
                # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ä»“åº“é“¾æ¥
                if is_valid_repo_url(href, ['github.com']):
                    print(f"      âœ… æ‰¾åˆ° GitHub é“¾æ¥: {href}")
                    return normalize_url(href)
        
        return None
        
    except Exception as e:
        print(f"      âš ï¸  è®¿é—®é¡¹ç›®é¡µé¢å¤±è´¥: {e}")
        return None


def filter_code_urls(urls: List[str]) -> List[str]:
    """
    è¿‡æ»¤å‡ºä»£ç ä»“åº“ URLï¼Œå¹¶ä»é¡¹ç›®ä¸»é¡µæå– GitHub é“¾æ¥
    
    Args:
        urls: URL åˆ—è¡¨
        
    Returns:
        ä»£ç ä»“åº“ URL åˆ—è¡¨
    """
    code_urls = []
    project_pages = []  # å¯èƒ½æ˜¯é¡¹ç›®ä¸»é¡µçš„ URL
    
    for url in urls:
        # ç›´æ¥æ˜¯ä»£ç ä»“åº“
        if is_valid_repo_url(url, CODE_HOST_DOMAINS):
            normalized = normalize_url(url)
            if normalized not in code_urls:
                code_urls.append(normalized)
        
        # å¯èƒ½æ˜¯é¡¹ç›®ä¸»é¡µï¼ˆå¸¸è§æ¨¡å¼ï¼‰
        elif any(pattern in url.lower() for pattern in [
            '.github.io',
            'github.io',
            'project',
            'page',
            'demo',
            'site'
        ]):
            # é¿å…æ˜æ˜¾ä¸æ˜¯é¡¹ç›®é¡µçš„é“¾æ¥
            if not any(skip in url.lower() for skip in ['arxiv.org', 'doi.org', 'youtube.com']):
                project_pages.append(url)
    
    # å¦‚æœæ²¡æœ‰ç›´æ¥æ‰¾åˆ°ä»£ç é“¾æ¥ï¼Œå°è¯•ä»é¡¹ç›®é¡µæå–
    if not code_urls and project_pages:
        print(f"    ğŸ” æœªæ‰¾åˆ°ç›´æ¥ä»£ç é“¾æ¥ï¼Œå°è¯•ä» {len(project_pages)} ä¸ªé¡¹ç›®é¡µé¢æå–...")
        
        for page_url in project_pages[:3]:  # æœ€å¤šå°è¯•3ä¸ª
            github_url = extract_github_from_project_page(page_url)
            if github_url and github_url not in code_urls:
                code_urls.append(github_url)
    
    return code_urls


def find_urls_with_context(text: str, urls: List[str]) -> List[Tuple[str, str]]:
    """
    æ‰¾åˆ° URL åŠå…¶ä¸Šä¸‹æ–‡ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦æ˜¯ä»£ç é“¾æ¥ï¼‰
    
    Args:
        text: æ–‡æœ¬å†…å®¹
        urls: URL åˆ—è¡¨
        
    Returns:
        (url, context) å…ƒç»„åˆ—è¡¨
    """
    results = []
    
    for url in urls:
        # æ‰¾åˆ° URL åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
        idx = text.find(url)
        if idx == -1:
            continue
        
        # æå–ä¸Šä¸‹æ–‡ï¼ˆå‰åå„100ä¸ªå­—ç¬¦ï¼‰
        start = max(0, idx - 100)
        end = min(len(text), idx + len(url) + 100)
        context = text[start:end]
        
        results.append((url, context))
    
    return results


def is_likely_code_url(context: str) -> bool:
    """
    æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ URL æ˜¯å¦å¯èƒ½æ˜¯ä»£ç é“¾æ¥
    
    Args:
        context: URL çš„ä¸Šä¸‹æ–‡
        
    Returns:
        æ˜¯å¦å¯èƒ½æ˜¯ä»£ç é“¾æ¥
    """
    context_lower = context.lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»£ç ç›¸å…³å…³é”®è¯
    for keyword in CODE_KEYWORDS:
        if keyword in context_lower:
            return True
    
    return False


def select_official_repo_with_llm(
    paper_data: Dict,
    candidate_urls: List[str]
) -> Optional[str]:
    """
    ä½¿ç”¨ LLM ä»å€™é€‰ URL ä¸­é€‰æ‹©æœ€å¯èƒ½çš„å®˜æ–¹å®ç°
    
    Args:
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        candidate_urls: å€™é€‰ URL åˆ—è¡¨
        
    Returns:
        é€‰ä¸­çš„ URLï¼Œå¦‚æœéƒ½ä¸æ˜¯è¿”å› None
    """
    if not candidate_urls:
        return None
    
    if len(candidate_urls) == 1:
        return candidate_urls[0]
    
    # æ„é€  prompt
    urls_text = "\n".join([f"{i+1}. {url}" for i, url in enumerate(candidate_urls)])
    
    messages = [
        {
            "role": "system",
            "content": "You are a tool that picks the most likely official code repository for a computer vision paper. Reply in JSON format."
        },
        {
            "role": "user",
            "content": f"""Paper title: "{paper_data.get('title', '')}"
Venue: {paper_data.get('conference', '')} {paper_data.get('year', '')}
Abstract: "{paper_data.get('abstract', '')[:500]}..."

Found URLs in PDF:
{urls_text}

From these URLs, which one is MOST likely the official implementation of the paper?
Reply in JSON format:
{{
  "selected_url": "<url or null>",
  "reason": "brief explanation"
}}"""
        }
    ]
    
    response = llm_client.call_json(messages)
    
    if response and "selected_url" in response:
        selected_url = response["selected_url"]
        if selected_url and selected_url.lower() != "null":
            print(f"    ğŸ¤– LLM é€‰æ‹©: {selected_url}")
            print(f"       ç†ç”±: {response.get('reason', 'N/A')}")
            return normalize_url(selected_url)
    
    return None


def extract_code_urls_from_pdf(
    pdf_path: str,
    paper_data: Dict,
    use_llm: bool = True
) -> Dict[str, any]:
    """
    ä» PDF ä¸­æå–ä»£ç ä»“åº“ URL
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM æ¥é€‰æ‹©å€™é€‰é“¾æ¥
        
    Returns:
        æå–ç»“æœå­—å…¸
    """
    print(f"  ğŸ“„ ä» PDF æå–ä»£ç é“¾æ¥...")
    
    # 1. æå–æ–‡æœ¬
    text = extract_text_from_pdf(pdf_path)
    if not text:
        return {
            "success": False,
            "official_repo_url": None,
            "candidates": [],
            "source": "pdf"
        }
    
    # 2. æå–æ‰€æœ‰ URL
    all_urls = extract_urls_from_text(text)
    print(f"    æ‰¾åˆ° {len(all_urls)} ä¸ª URL")
    
    # 3. è¿‡æ»¤å‡ºä»£ç ä»“åº“ URL
    code_urls = filter_code_urls(all_urls)
    print(f"    å…¶ä¸­ {len(code_urls)} ä¸ªæ˜¯ä»£ç ä»“åº“ URL")
    
    if not code_urls:
        return {
            "success": False,
            "official_repo_url": None,
            "candidates": [],
            "source": "pdf"
        }
    
    # 4. æ ¹æ®ä¸Šä¸‹æ–‡è¿‡æ»¤
    urls_with_context = find_urls_with_context(text, code_urls)
    likely_code_urls = [
        url for url, context in urls_with_context
        if is_likely_code_url(context)
    ]
    
    # å¦‚æœæœ‰æ˜ç¡®çš„ä»£ç ç›¸å…³ URLï¼Œä¼˜å…ˆä½¿ç”¨
    candidates = likely_code_urls if likely_code_urls else code_urls
    print(f"    ç»ä¸Šä¸‹æ–‡åˆ†æï¼Œ{len(candidates)} ä¸ªå€™é€‰é“¾æ¥")
    
    # 5. é€‰æ‹©å®˜æ–¹ä»“åº“
    official_url = None
    
    if len(candidates) == 1:
        official_url = candidates[0]
        print(f"    âœ… æ‰¾åˆ°å”¯ä¸€å€™é€‰: {official_url}")
    elif len(candidates) > 1 and use_llm:
        official_url = select_official_repo_with_llm(paper_data, candidates)
    elif len(candidates) > 1:
        # ä¸ä½¿ç”¨ LLMï¼Œé€‰ç¬¬ä¸€ä¸ª
        official_url = candidates[0]
        print(f"    âš ï¸  æœ‰å¤šä¸ªå€™é€‰ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª: {official_url}")
    
    return {
        "success": bool(official_url),
        "official_repo_url": official_url,
        "candidates": candidates,
        "source": "pdf"
    }


def process_paper_pdf(paper_dir: str, paper_data: Dict, use_llm: bool = True) -> Dict:
    """
    å¤„ç†å•ç¯‡è®ºæ–‡çš„ PDF æå–
    
    Args:
        paper_dir: è®ºæ–‡ç›®å½•
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM
        
    Returns:
        æå–ç»“æœ
    """
    pdf_path = os.path.join(paper_dir, "paper.pdf")
    
    if not os.path.exists(pdf_path):
        print(f"    âš ï¸  PDF æ–‡ä»¶ä¸å­˜åœ¨")
        return {
            "success": False,
            "official_repo_url": None,
            "candidates": [],
            "source": "pdf"
        }
    
    return extract_code_urls_from_pdf(pdf_path, paper_data, use_llm)


if __name__ == "__main__":
    # æµ‹è¯•
    from config import PAPERS_ROOT_DIR
    from utils import get_all_paper_dirs, load_paper_data
    
    print("æµ‹è¯• PDF æå–å™¨...")
    paper_dirs = get_all_paper_dirs(PAPERS_ROOT_DIR)
    
    if paper_dirs:
        test_dir = paper_dirs[0]
        print(f"\næµ‹è¯•è®ºæ–‡: {os.path.basename(test_dir)}")
        
        paper_data = load_paper_data(test_dir)
        if paper_data:
            result = process_paper_pdf(test_dir, paper_data, use_llm=True)
            print(f"\nç»“æœ: {result}")
