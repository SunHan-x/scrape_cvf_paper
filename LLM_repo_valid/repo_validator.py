"""
ä»“åº“è´¨é‡éªŒè¯å™¨ - è¯„ä¼°ä»£ç ä»“åº“æ˜¯å¦æœ‰æ„ä¹‰ä¸”ç»´æŠ¤è‰¯å¥½
"""

import os
import time
from typing import Dict, Optional, List
from datetime import datetime
import requests

from config import (
    MIN_CODE_FILES, MIN_REPO_SIZE_KB, MAX_ABANDONED_YEARS,
    MIN_STARS_FOR_OLD_REPO, CODE_EXTENSIONS, TYPICAL_IMPL_FILES,
    TYPICAL_IMPL_DIRS, MAX_DEPTH, MAX_FILES_TO_ANALYZE, SAMPLE_FILE_LINES
)
from utils import extract_repo_owner_name, truncate_text
from llm_client import llm_client
from github_search import GitHubSearcher


def get_repo_file_tree(owner: str, repo: str, path: str = "") -> Optional[List[Dict]]:
    """
    è·å–ä»“åº“æ–‡ä»¶æ ‘
    
    Args:
        owner: ä»“åº“æ‰€æœ‰è€…
        repo: ä»“åº“åç§°
        path: è·¯å¾„ï¼ˆé»˜è®¤æ ¹ç›®å½•ï¼‰
        
    Returns:
        æ–‡ä»¶åˆ—è¡¨
    """
    searcher = GitHubSearcher()
    url = f"{searcher.base_url}/repos/{owner}/{repo}/contents/{path}"
    
    response = searcher._make_request(url)
    
    if not response or response.status_code != 200:
        return None
    
    try:
        return response.json()
    except Exception as e:
        print(f"    âš ï¸  è·å–æ–‡ä»¶æ ‘å¤±è´¥: {e}")
        return None


def get_readme_content(owner: str, repo: str) -> Optional[str]:
    """
    è·å– README æ–‡ä»¶å†…å®¹
    
    Args:
        owner: ä»“åº“æ‰€æœ‰è€…
        repo: ä»“åº“åç§°
        
    Returns:
        README å†…å®¹
    """
    searcher = GitHubSearcher()
    url = f"{searcher.base_url}/repos/{owner}/{repo}/readme"
    
    response = searcher._make_request(url)
    
    if not response or response.status_code != 200:
        return None
    
    try:
        data = response.json()
        
        # README å†…å®¹æ˜¯ base64 ç¼–ç çš„
        import base64
        content = base64.b64decode(data["content"]).decode("utf-8")
        return content
        
    except Exception as e:
        print(f"    âš ï¸  è·å– README å¤±è´¥: {e}")
        return None


def analyze_file_structure(files: List[Dict]) -> Dict:
    """
    åˆ†ææ–‡ä»¶ç»“æ„
    
    Args:
        files: æ–‡ä»¶åˆ—è¡¨
        
    Returns:
        åˆ†æç»“æœ
    """
    code_files = []
    non_code_files = []
    directories = []
    
    for item in files:
        name = item.get("name", "")
        item_type = item.get("type", "")
        
        if item_type == "dir":
            directories.append(name)
        elif item_type == "file":
            ext = os.path.splitext(name)[1].lower()
            if ext in CODE_EXTENSIONS:
                code_files.append(name)
            else:
                non_code_files.append(name)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…¸å‹å®ç°æ–‡ä»¶
    has_typical_files = any(
        f in code_files for f in TYPICAL_IMPL_FILES
    )
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…¸å‹å®ç°ç›®å½•
    has_typical_dirs = any(
        d in directories for d in TYPICAL_IMPL_DIRS
    )
    
    return {
        "code_file_count": len(code_files),
        "code_files": code_files,
        "non_code_file_count": len(non_code_files),
        "directory_count": len(directories),
        "directories": directories,
        "has_typical_files": has_typical_files,
        "has_typical_dirs": has_typical_dirs,
    }


def get_file_content(owner: str, repo: str, path: str) -> Optional[str]:
    """
    è·å–æ–‡ä»¶å†…å®¹
    
    Args:
        owner: ä»“åº“æ‰€æœ‰è€…
        repo: ä»“åº“åç§°
        path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ–‡ä»¶å†…å®¹
    """
    searcher = GitHubSearcher()
    url = f"{searcher.base_url}/repos/{owner}/{repo}/contents/{path}"
    
    response = searcher._make_request(url)
    
    if not response or response.status_code != 200:
        return None
    
    try:
        data = response.json()
        
        # æ–‡ä»¶å†…å®¹æ˜¯ base64 ç¼–ç çš„
        import base64
        content = base64.b64decode(data["content"]).decode("utf-8", errors="ignore")
        return content
        
    except Exception:
        return None


def deep_analyze_repo_structure(owner: str, repo: str, max_depth: int = MAX_DEPTH) -> Dict:
    """
    æ·±åº¦åˆ†æä»“åº“ç»“æ„
    
    Args:
        owner: ä»“åº“æ‰€æœ‰è€…
        repo: ä»“åº“åç§°
        max_depth: æœ€å¤§éå†æ·±åº¦
        
    Returns:
        æ·±åº¦åˆ†æç»“æœ
    """
    all_code_files = []
    all_directories = []
    file_tree = {}
    
    def traverse_dir(path: str, depth: int):
        """é€’å½’éå†ç›®å½•"""
        if depth > max_depth:
            return
        
        files = get_repo_file_tree(owner, repo, path)
        if not files:
            return
        
        for item in files:
            name = item.get("name", "")
            item_type = item.get("type", "")
            full_path = f"{path}/{name}" if path else name
            
            if item_type == "dir":
                all_directories.append(full_path)
                # é€’å½’éå†å­ç›®å½•
                traverse_dir(full_path, depth + 1)
            elif item_type == "file":
                ext = os.path.splitext(name)[1].lower()
                if ext in CODE_EXTENSIONS:
                    all_code_files.append({
                        "path": full_path,
                        "name": name,
                        "extension": ext,
                        "size": item.get("size", 0)
                    })
    
    # ä»æ ¹ç›®å½•å¼€å§‹éå†
    traverse_dir("", 0)
    
    # åˆ†æä»£ç æ–‡ä»¶åˆ†å¸ƒ
    extension_counts = {}
    for file in all_code_files:
        ext = file["extension"]
        extension_counts[ext] = extension_counts.get(ext, 0) + 1
    
    # è¯†åˆ«ä¸»è¦è¯­è¨€
    main_language = max(extension_counts.items(), key=lambda x: x[1])[0] if extension_counts else None
    
    # åˆ†æç›®å½•ç»“æ„
    has_models_dir = any("model" in d.lower() for d in all_directories)
    has_data_dir = any("data" in d.lower() or "dataset" in d.lower() for d in all_directories)
    has_train_dir = any("train" in d.lower() for d in all_directories)
    has_test_dir = any("test" in d.lower() for d in all_directories)
    has_config_dir = any("config" in d.lower() for d in all_directories)
    
    # è¯†åˆ«å…³é”®ä»£ç æ–‡ä»¶
    key_files = []
    for file in all_code_files:
        name_lower = file["name"].lower()
        if any(keyword in name_lower for keyword in ["train", "model", "network", "main", "run"]):
            key_files.append(file)
    
    return {
        "total_code_files": len(all_code_files),
        "total_directories": len(all_directories),
        "extension_counts": extension_counts,
        "main_language": main_language,
        "has_models_dir": has_models_dir,
        "has_data_dir": has_data_dir,
        "has_train_dir": has_train_dir,
        "has_test_dir": has_test_dir,
        "has_config_dir": has_config_dir,
        "key_files": key_files[:10],  # åªä¿ç•™å‰10ä¸ªå…³é”®æ–‡ä»¶
        "all_code_files": all_code_files[:MAX_FILES_TO_ANALYZE],  # é™åˆ¶æ•°é‡
    }


def sample_code_files(owner: str, repo: str, files: List[Dict], max_samples: int = 5) -> List[Dict]:
    """
    é‡‡æ ·å…³é”®ä»£ç æ–‡ä»¶å†…å®¹
    
    Args:
        owner: ä»“åº“æ‰€æœ‰è€…
        repo: ä»“åº“åç§°
        files: æ–‡ä»¶åˆ—è¡¨
        max_samples: æœ€å¤§é‡‡æ ·æ•°
        
    Returns:
        é‡‡æ ·çš„æ–‡ä»¶å†…å®¹
    """
    samples = []
    
    # ä¼˜å…ˆé€‰æ‹©å…³é”®æ–‡ä»¶
    priority_keywords = ["train", "model", "network", "main"]
    
    # å…ˆé€‰æ‹©ä¼˜å…ˆçº§é«˜çš„æ–‡ä»¶
    priority_files = [f for f in files if any(kw in f["name"].lower() for kw in priority_keywords)]
    other_files = [f for f in files if f not in priority_files]
    
    selected_files = (priority_files + other_files)[:max_samples]
    
    for file in selected_files:
        content = get_file_content(owner, repo, file["path"])
        if content:
            # åªå–å‰Nè¡Œ
            lines = content.split("\n")[:SAMPLE_FILE_LINES]
            samples.append({
                "path": file["path"],
                "name": file["name"],
                "lines": len(content.split("\n")),
                "sample": "\n".join(lines)
            })
    
    return samples


def rule_based_filter(repo_info: Dict, paper_year: int) -> Dict:
    """
    åŸºäºè§„åˆ™çš„å¿«é€Ÿè¿‡æ»¤
    
    Args:
        repo_info: ä»“åº“ä¿¡æ¯
        paper_year: è®ºæ–‡å‘è¡¨å¹´ä»½
        
    Returns:
        è¿‡æ»¤ç»“æœ
    """
    # è·å–æ–‡ä»¶æ ‘
    owner_repo = extract_repo_owner_name(repo_info["html_url"])
    if not owner_repo:
        return {
            "is_meaningful": False,
            "confident": True,
            "reason": "Invalid repository URL",
            "score": 0.0
        }
    
    owner, repo = owner_repo
    
    # è·å–æ ¹ç›®å½•æ–‡ä»¶
    files = get_repo_file_tree(owner, repo)
    if files is None:
        return {
            "is_meaningful": None,
            "confident": False,
            "reason": "Cannot fetch repository contents",
            "score": None
        }
    
    # åˆ†ææ–‡ä»¶ç»“æ„
    structure = analyze_file_structure(files)
    
    # è§„åˆ™ 1: æ²¡æœ‰ä»£ç æ–‡ä»¶
    if structure["code_file_count"] == 0:
        return {
            "is_meaningful": False,
            "confident": True,
            "reason": "No code files found",
            "score": 0.0,
            "structure": structure
        }
    
    # è§„åˆ™ 2: ä»“åº“å¤ªå°ä¸”ä»£ç æ–‡ä»¶å¾ˆå°‘
    if repo_info["size"] < MIN_REPO_SIZE_KB and structure["code_file_count"] <= 1:
        return {
            "is_meaningful": False,
            "confident": True,
            "reason": f"Very tiny repo (size: {repo_info['size']}KB) with almost no code",
            "score": 0.1,
            "structure": structure
        }
    
    # è§„åˆ™ 3: å·²å½’æ¡£
    if repo_info.get("archived", False):
        return {
            "is_meaningful": False,
            "confident": True,
            "reason": "Repository is archived",
            "score": 0.2,
            "structure": structure
        }
    
    # è§„åˆ™ 4: é•¿æ—¶é—´æœªæ›´æ–°ä¸”æ²¡æœ‰å…³æ³¨
    try:
        last_push = datetime.strptime(repo_info["pushed_at"], "%Y-%m-%dT%H:%M:%SZ")
        current_year = datetime.now().year
        years_since_push = current_year - last_push.year
        
        if years_since_push > MAX_ABANDONED_YEARS and repo_info["stars"] < MIN_STARS_FOR_OLD_REPO:
            return {
                "is_meaningful": False,
                "confident": True,
                "reason": f"Abandoned repo (last push: {years_since_push} years ago, stars: {repo_info['stars']})",
                "score": 0.2,
                "structure": structure
            }
    except Exception:
        pass
    
    # è§„åˆ™é€šè¿‡ï¼Œä½†ä¸å®Œå…¨ç¡®å®šï¼ˆéœ€è¦ LLM æ·±åº¦è¯„ä¼°ï¼‰
    return {
        "is_meaningful": True,
        "confident": False,
        "reason": "Has code files and basic structure",
        "score": None,
        "structure": structure
    }


def llm_evaluate_repo(
    repo_info: Dict,
    paper_data: Dict,
    structure: Dict,
    readme: Optional[str] = None,
    deep_structure: Optional[Dict] = None,
    code_samples: Optional[List[Dict]] = None
) -> Dict:
    """
    ä½¿ç”¨ LLM æ·±åº¦è¯„ä¼°ä»“åº“è´¨é‡
    
    Args:
        repo_info: ä»“åº“ä¿¡æ¯
        paper_data: è®ºæ–‡æ•°æ®
        structure: æ–‡ä»¶ç»“æ„åˆ†æ
        readme: README å†…å®¹
        deep_structure: æ·±åº¦ç»“æ„åˆ†æ
        code_samples: ä»£ç é‡‡æ ·
        
    Returns:
        è¯„ä¼°ç»“æœ
    """
    print(f"  ğŸ¤– LLM æ·±åº¦è¯„ä¼°ä»“åº“è´¨é‡...")
    
    # å¦‚æœæ²¡æœ‰ READMEï¼Œå°è¯•è·å–
    if readme is None:
        owner_repo = extract_repo_owner_name(repo_info["html_url"])
        if owner_repo:
            owner, repo = owner_repo
            readme = get_readme_content(owner, repo)
    
    # æ„é€ æ–‡ä»¶æ ‘æ–‡æœ¬
    tree_text = "æ ¹ç›®å½•:\n"
    tree_text += f"  ä»£ç æ–‡ä»¶ ({structure['code_file_count']}): {', '.join(structure['code_files'][:10])}\n"
    tree_text += f"  ç›®å½• ({structure['directory_count']}): {', '.join(structure['directories'][:10])}\n"
    
    # æ·»åŠ æ·±åº¦ç»“æ„ä¿¡æ¯
    if deep_structure:
        tree_text += f"\næ·±åº¦åˆ†æ (éå†äº† {MAX_DEPTH} å±‚ç›®å½•):\n"
        tree_text += f"  æ€»ä»£ç æ–‡ä»¶æ•°: {deep_structure['total_code_files']}\n"
        tree_text += f"  æ–‡ä»¶ç±»å‹åˆ†å¸ƒ: {deep_structure['extension_counts']}\n"
        tree_text += f"  ä¸»è¦è¯­è¨€: {deep_structure['main_language']}\n"
        tree_text += f"  ç›®å½•ç»“æ„:\n"
        tree_text += f"    - æ¨¡å‹ç›®å½•: {'æœ‰' if deep_structure['has_models_dir'] else 'æ— '}\n"
        tree_text += f"    - æ•°æ®ç›®å½•: {'æœ‰' if deep_structure['has_data_dir'] else 'æ— '}\n"
        tree_text += f"    - è®­ç»ƒç›®å½•: {'æœ‰' if deep_structure['has_train_dir'] else 'æ— '}\n"
        tree_text += f"    - æµ‹è¯•ç›®å½•: {'æœ‰' if deep_structure['has_test_dir'] else 'æ— '}\n"
        tree_text += f"    - é…ç½®ç›®å½•: {'æœ‰' if deep_structure['has_config_dir'] else 'æ— '}\n"
        
        if deep_structure['key_files']:
            tree_text += f"  å…³é”®æ–‡ä»¶:\n"
            for file in deep_structure['key_files'][:5]:
                tree_text += f"    - {file['path']}\n"
    
    # æ·»åŠ ä»£ç é‡‡æ ·
    code_samples_text = ""
    if code_samples:
        code_samples_text = "\nä»£ç ç¤ºä¾‹:\n"
        for sample in code_samples[:3]:  # æœ€å¤š3ä¸ªç¤ºä¾‹
            code_samples_text += f"\næ–‡ä»¶: {sample['path']} (å…± {sample['lines']} è¡Œ)\n"
            code_samples_text += f"```\n{sample['sample']}\n```\n"
    
    # æˆªæ–­ README
    readme_text = truncate_text(readme, 1000) if readme else "æ—  README"
    
    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆã€‚è¯·è¯„ä¼° GitHub ä»“åº“æ˜¯å¦æ˜¯ä¸€ä¸ªæœ‰æ„ä¹‰ã€ç»´æŠ¤è‰¯å¥½çš„å®ç°ã€‚è¯·ç”¨ JSON æ ¼å¼å›å¤ï¼Œå¹¶ä½¿ç”¨ä¸­æ–‡ã€‚"
        },
        {
            "role": "user",
            "content": f"""è®ºæ–‡æ ‡é¢˜: "{paper_data.get('title', '')}"
å¹´ä»½: {paper_data.get('year', '')}
æ‘˜è¦: "{truncate_text(paper_data.get('abstract', ''), 300)}"

ä»“åº“: {repo_info['html_url']}

åŸºæœ¬ç»Ÿè®¡:
- Stars: {repo_info['stars']}
- Forks: {repo_info['forks']}
- æœ€åæäº¤: {repo_info['pushed_at']}
- ä¸»è¦è¯­è¨€: {repo_info['language']}
- å¤§å°: {repo_info['size']}KB
- æ˜¯å¦å·²å½’æ¡£: {repo_info.get('archived', False)}
- ä»£ç æ–‡ä»¶æ•°: {structure['code_file_count']}
- æ˜¯å¦æœ‰å…¸å‹ç»“æ„: {structure['has_typical_files'] or structure['has_typical_dirs']}

{tree_text}

README (æˆªæ–­):
{readme_text}

{code_samples_text}

è¯·ä»”ç»†åˆ†æä»“åº“çš„ä»£ç æ¶æ„ã€å®ç°è´¨é‡å’Œç»´æŠ¤çŠ¶æ€ï¼Œå¹¶ç”¨ JSON æ ¼å¼å›å¤ï¼š
{{
  "is_meaningful": true/false,
  "is_implementation_of_paper": true/false,
  "has_complete_architecture": true/false,
  "code_organization_score": 0.0-1.0,
  "maintenance_score": 0.0-1.0,
  "code_quality_score": 0.0-1.0,
  "overall_score": 0.0-1.0,
  "architecture_analysis": "ä»£ç æ¶æ„åˆ†æï¼ˆä¸­æ–‡ï¼‰",
  "reasons": ["åŸå› 1ï¼ˆä¸­æ–‡ï¼‰", "åŸå› 2ï¼ˆä¸­æ–‡ï¼‰", ...]
}}"""
        }
    ]
    
    response = llm_client.call_json(messages, temperature=0.1)
    
    if not response:
        return {
            "is_meaningful": None,
            "score": None,
            "reason": "LLM evaluation failed"
        }
    
    return {
        "is_meaningful": response.get("is_meaningful", False),
        "is_implementation": response.get("is_implementation_of_paper", False),
        "has_complete_architecture": response.get("has_complete_architecture", False),
        "code_organization_score": response.get("code_organization_score", 0.0),
        "maintenance_score": response.get("maintenance_score", 0.0),
        "code_quality_score": response.get("code_quality_score", 0.0),
        "score": response.get("overall_score", 0.0),
        "architecture_analysis": response.get("architecture_analysis", ""),
        "reasons": response.get("reasons", [])
    }


def validate_repository(
    repo_url: str,
    paper_data: Dict,
    use_llm: bool = True
) -> Dict:
    """
    éªŒè¯ä»“åº“æ˜¯å¦æœ‰æ„ä¹‰
    
    Args:
        repo_url: ä»“åº“ URL
        paper_data: è®ºæ–‡æ•°æ®
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM æ·±åº¦è¯„ä¼°
        
    Returns:
        éªŒè¯ç»“æœ
    """
    print(f"  âœ… éªŒè¯ä»“åº“: {repo_url}")
    
    # è·å–ä»“åº“ä¿¡æ¯
    owner_repo = extract_repo_owner_name(repo_url)
    if not owner_repo:
        return {
            "is_meaningful": False,
            "score": 0.0,
            "reason": "Invalid repository URL"
        }
    
    owner, repo = owner_repo
    searcher = GitHubSearcher()
    repo_info = searcher.get_repo_info(owner, repo)
    
    if not repo_info:
        return {
            "is_meaningful": False,
            "score": 0.0,
            "reason": "Repository not found or inaccessible"
        }
    
    # è§„åˆ™è¿‡æ»¤
    rule_result = rule_based_filter(repo_info, paper_data.get("year", 2020))
    
    print(f"    è§„åˆ™è¯„ä¼°: {rule_result['reason']}")
    
    # å¦‚æœè§„åˆ™å·²ç»å¾ˆç¡®å®šï¼Œç›´æ¥è¿”å›
    if rule_result["confident"]:
        return rule_result
    
    # éœ€è¦ LLM æ·±åº¦è¯„ä¼°
    if use_llm:
        print(f"    å¼€å§‹æ·±åº¦åˆ†æä»“åº“ç»“æ„...")
        
        # æ·±åº¦åˆ†æä»“åº“ç»“æ„
        deep_structure = deep_analyze_repo_structure(owner, repo)
        print(f"    å‘ç° {deep_structure['total_code_files']} ä¸ªä»£ç æ–‡ä»¶")
        
        # é‡‡æ ·ä»£ç æ–‡ä»¶
        code_samples = None
        if deep_structure['all_code_files']:
            print(f"    é‡‡æ ·å…³é”®ä»£ç æ–‡ä»¶...")
            code_samples = sample_code_files(owner, repo, deep_structure['all_code_files'], max_samples=5)
            print(f"    æˆåŠŸé‡‡æ · {len(code_samples)} ä¸ªæ–‡ä»¶")
        
        # LLM è¯„ä¼°
        llm_result = llm_evaluate_repo(
            repo_info,
            paper_data,
            rule_result.get("structure", {}),
            None,
            deep_structure,
            code_samples
        )
        
        print(f"    LLM è¯„ä¼°åˆ†æ•°: {llm_result.get('score', 'N/A')}")
        if llm_result.get('architecture_analysis'):
            print(f"    æ¶æ„åˆ†æ: {llm_result['architecture_analysis'][:100]}...")
        print(f"    åŸå› : {', '.join(llm_result.get('reasons', []))}")
        
        return {
            **rule_result,
            **llm_result,
            "confident": True
        }
    
    # ä¸ä½¿ç”¨ LLMï¼Œè¿”å›è§„åˆ™ç»“æœ
    return rule_result


if __name__ == "__main__":
    # æµ‹è¯•
    test_repo_url = "https://github.com/some/repo"
    test_paper_data = {
        "title": "Test Paper",
        "year": 2024,
        "abstract": "This is a test abstract."
    }
    
    result = validate_repository(test_repo_url, test_paper_data, use_llm=True)
    print(f"\nç»“æœ: {result}")
