"""
GitHub æœç´¢å™¨ - ä½¿ç”¨ GitHub API æœç´¢è®ºæ–‡ç›¸å…³å®ç°
"""

import time
from typing import List, Dict, Optional
import requests

from config import GITHUB_API_TOKEN
from utils import normalize_url, extract_repo_owner_name
from llm_client import llm_client


class GitHubSearcher:
    """GitHub API æœç´¢å°è£…"""
    
    def __init__(self, token: Optional[str] = GITHUB_API_TOKEN):
        self.token = token
        self.base_url = "https://api.github.com"
        self.headers = {
            "Accept": "application/vnd.github.v3+json"
        }
        if token and token != "your_github_token_here":
            self.headers["Authorization"] = f"token {token}"
    
    def check_rate_limit(self) -> Dict:
        """æ£€æŸ¥ API é€Ÿç‡é™åˆ¶çŠ¶æ€"""
        try:
            response = requests.get(
                f"{self.base_url}/rate_limit",
                headers=self.headers,
                timeout=10
            )
            if response.status_code == 200:
                data = response.json()
                return data.get("rate", {})
        except Exception:
            pass
        return {}
    
    def wait_for_rate_limit(self, retry_after: Optional[int] = None):
        """ç­‰å¾…é€Ÿç‡é™åˆ¶æ¢å¤"""
        if retry_after:
            wait_time = retry_after + 5  # é¢å¤–ç­‰å¾… 5 ç§’
        else:
            rate_limit = self.check_rate_limit()
            remaining = rate_limit.get("remaining", 0)
            reset_time = rate_limit.get("reset", 0)
            
            if remaining == 0 and reset_time:
                wait_time = max(reset_time - time.time() + 5, 60)
            else:
                wait_time = 60
        
        print(f"    â³ ç­‰å¾… {wait_time:.0f} ç§’åé‡è¯•...")
        time.sleep(wait_time)
    
    def _make_request(self, url: str, params: Optional[Dict] = None, max_retries: int = 3) -> Optional[requests.Response]:
        """å‘èµ· HTTP è¯·æ±‚ï¼Œå¸¦æœ‰é‡è¯•å’Œé€Ÿç‡é™åˆ¶å¤„ç†"""
        for attempt in range(max_retries):
            try:
                response = requests.get(
                    url,
                    headers=self.headers,
                    params=params,
                    timeout=10
                )
                
                # å¤„ç†é€Ÿç‡é™åˆ¶
                if response.status_code == 403:
                    retry_after = response.headers.get("Retry-After")
                    if "rate limit" in response.text.lower():
                        print(f"    âš ï¸  GitHub API é€Ÿç‡é™åˆ¶ (å°è¯• {attempt + 1}/{max_retries})")
                        if attempt < max_retries - 1:
                            self.wait_for_rate_limit(int(retry_after) if retry_after else None)
                            continue
                        else:
                            return None
                    else:
                        return response
                
                return response
                
            except requests.exceptions.Timeout:
                print(f"    âš ï¸  è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
            except requests.exceptions.RequestException as e:
                print(f"    âš ï¸  è¯·æ±‚å¤±è´¥: {e} (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
        
        return None
    
    def search_repositories(
        self,
        query: str,
        max_results: int = 10
    ) -> List[Dict]:
        """
        æœç´¢ GitHub ä»“åº“
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            ä»“åº“ä¿¡æ¯åˆ—è¡¨
        """
        url = f"{self.base_url}/search/repositories"
        params = {
            "q": query,
            "sort": "stars",
            "order": "desc",
            "per_page": min(max_results, 100)
        }
        
        response = self._make_request(url, params)
        
        if not response or response.status_code != 200:
            print(f"    âŒ GitHub æœç´¢å¤±è´¥")
            return []
        
        try:
            data = response.json()
            
            repos = []
            for item in data.get("items", [])[:max_results]:
                repos.append({
                    "name": item["name"],
                    "full_name": item["full_name"],
                    "html_url": item["html_url"],
                    "description": item.get("description", ""),
                    "stars": item["stargazers_count"],
                    "forks": item["forks_count"],
                    "language": item.get("language", ""),
                    "updated_at": item["updated_at"],
                    "topics": item.get("topics", []),
                })
            
            return repos
            
        except Exception as e:
            print(f"    âŒ è§£æå“åº”å¤±è´¥: {e}")
            return []
    
    def get_repo_info(self, owner: str, repo: str) -> Optional[Dict]:
        """
        è·å–ä»“åº“è¯¦ç»†ä¿¡æ¯
        
        Args:
            owner: ä»“åº“æ‰€æœ‰è€…
            repo: ä»“åº“åç§°
            
        Returns:
            ä»“åº“ä¿¡æ¯å­—å…¸
        """
        url = f"{self.base_url}/repos/{owner}/{repo}"
        
        response = self._make_request(url)
        
        if not response:
            return None
        
        if response.status_code == 404:
            return None
        
        if response.status_code != 200:
            print(f"    âŒ è·å–ä»“åº“ä¿¡æ¯å¤±è´¥: HTTP {response.status_code}")
            return None
        
        try:
            data = response.json()
            
            return {
                "name": data["name"],
                "full_name": data["full_name"],
                "html_url": data["html_url"],
                "description": data.get("description", ""),
                "stars": data["stargazers_count"],
                "forks": data["forks_count"],
                "language": data.get("language", ""),
                "size": data["size"],
                "created_at": data["created_at"],
                "updated_at": data["updated_at"],
                "pushed_at": data["pushed_at"],
                "topics": data.get("topics", []),
                "archived": data.get("archived", False),
                "license": data.get("license", {}).get("name", None) if data.get("license") else None,
                "open_issues": data.get("open_issues_count", 0),
            }
            
        except Exception as e:
            print(f"    âŒ è§£æä»“åº“ä¿¡æ¯å¤±è´¥: {e}")
            return None


def construct_search_query(paper_data: Dict) -> str:
    """
    æ ¹æ®è®ºæ–‡æ•°æ®æ„é€  GitHub æœç´¢æŸ¥è¯¢
    
    Args:
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        
    Returns:
        æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
    """
    title = paper_data.get("title", "")
    
    # æå–ä¸»è¦å…³é”®è¯ï¼ˆå»æ‰å† è¯ã€ä»‹è¯ç­‰ï¼‰
    stop_words = {"a", "an", "the", "of", "for", "with", "on", "in", "to", "and", "or"}
    keywords = [
        word for word in title.split()
        if word.lower() not in stop_words and len(word) > 2
    ][:5]  # å–å‰5ä¸ªå…³é”®è¯
    
    # æ„é€ æŸ¥è¯¢
    query_parts = [f'"{title}"']
    
    # æ·»åŠ  in:name,description,readme æ¥æé«˜ç›¸å…³æ€§
    query = f'{" ".join(query_parts)} in:name,description,readme'
    
    return query


def filter_implementations_with_llm(
    paper_data: Dict,
    candidates: List[Dict]
) -> List[Dict]:
    """
    ä½¿ç”¨ LLM è¿‡æ»¤å‡ºçœŸæ­£å®ç°äº†è®ºæ–‡çš„ä»“åº“
    
    Args:
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        candidates: å€™é€‰ä»“åº“åˆ—è¡¨
        
    Returns:
        è¿‡æ»¤åçš„ä»“åº“åˆ—è¡¨ï¼ŒæŒ‰ç›¸å…³æ€§æ’åº
    """
    if not candidates:
        return []
    
    # æ„é€ å€™é€‰åˆ—è¡¨æ–‡æœ¬
    candidates_text = []
    for i, repo in enumerate(candidates):
        text = f"{i+1}. {repo['full_name']} - {repo['html_url']}\n"
        text += f"   Description: {repo['description']}\n"
        text += f"   Stars: {repo['stars']}, Language: {repo['language']}"
        candidates_text.append(text)
    
    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œç”¨äºè¯†åˆ« GitHub ä»“åº“æ˜¯å¦æ˜¯æŸç¯‡ç ”ç©¶è®ºæ–‡çš„å®ç°ä»£ç ã€‚è¯·ç”¨ JSON æ ¼å¼å›å¤ï¼Œå¹¶ä½¿ç”¨ä¸­æ–‡ã€‚"
        },
        {
            "role": "user",
            "content": f"""è®ºæ–‡æ ‡é¢˜: "{paper_data.get('title', '')}"
å¹´ä»½: {paper_data.get('year', '')}
ä¼šè®®: {paper_data.get('conference', '')}
æ‘˜è¦: "{paper_data.get('abstract', '')[:500]}..."

ä»¥ä¸‹æ˜¯ GitHub æœç´¢ç»“æœä¸­çš„ä»“åº“ï¼š
{chr(10).join(candidates_text)}

å¯¹äºæ¯ä¸ªä»“åº“ï¼Œåˆ¤æ–­å®ƒæ˜¯å¦æ˜¯è¿™ç¯‡è®ºæ–‡çš„å®ç°ä»£ç ï¼ˆæˆ–éå¸¸æ¥è¿‘çš„é‡æ–°å®ç°ï¼‰ã€‚
è¯·ç”¨ JSON æ ¼å¼å›å¤ï¼ŒåŒ…å«ä¸€ä¸ªå¯¹è±¡æ•°ç»„ï¼š
{{
  "repositories": [
    {{
      "full_name": "<ä»“åº“å…¨å>",
      "url": "<ä»“åº“ URL>",
      "is_implementation": true/false,
      "relevance": 0.0-1.0,
      "reason": "ç®€è¦è¯´æ˜åŸå› ï¼ˆä¸­æ–‡ï¼‰"
    }},
    ...
  ]
}}"""
        }
    ]
    
    response = llm_client.call_json(messages, temperature=0.1)
    
    if not response or "repositories" not in response:
        print(f"    âš ï¸  LLM å“åº”æ ¼å¼é”™è¯¯")
        return []
    
    # è¿‡æ»¤å¹¶æ’åº
    filtered = []
    for item in response["repositories"]:
        if item.get("is_implementation", False) and item.get("relevance", 0) > 0.3:
            # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹ä»“åº“æ•°æ®
            for candidate in candidates:
                if candidate["full_name"] == item["full_name"]:
                    filtered.append({
                        **candidate,
                        "relevance": item["relevance"],
                        "reason": item.get("reason", "")
                    })
                    break
    
    # æŒ‰ç›¸å…³æ€§æ’åº
    filtered.sort(key=lambda x: x["relevance"], reverse=True)
    
    return filtered


def search_github_implementations(
    paper_data: Dict,
    max_results: int = 10,
    use_llm: bool = True
) -> Dict:
    """
    åœ¨ GitHub ä¸Šæœç´¢è®ºæ–‡çš„å®ç°
    
    Args:
        paper_data: è®ºæ–‡å…ƒæ•°æ®
        max_results: æœ€å¤§æœç´¢ç»“æœæ•°
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM è¿‡æ»¤
        
    Returns:
        æœç´¢ç»“æœå­—å…¸
    """
    print(f"  ğŸ” åœ¨ GitHub æœç´¢å®ç°...")
    
    searcher = GitHubSearcher()
    
    # æ„é€ æŸ¥è¯¢
    query = construct_search_query(paper_data)
    print(f"    æŸ¥è¯¢: {query}")
    
    # æœç´¢
    candidates = searcher.search_repositories(query, max_results)
    print(f"    æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰ä»“åº“")
    
    if not candidates:
        return {
            "success": False,
            "unofficial_repos": [],
            "source": "github_search"
        }
    
    # LLM è¿‡æ»¤
    if use_llm:
        filtered = filter_implementations_with_llm(paper_data, candidates)
        print(f"    LLM è¿‡æ»¤åä¿ç•™ {len(filtered)} ä¸ªå®ç°")
        
        if filtered:
            for i, repo in enumerate(filtered[:3], 1):
                print(f"      {i}. {repo['full_name']} (ç›¸å…³æ€§: {repo['relevance']:.2f})")
                print(f"         {repo.get('reason', 'N/A')}")
    else:
        filtered = candidates
    
    return {
        "success": bool(filtered),
        "unofficial_repos": [repo["html_url"] for repo in filtered],
        "repo_details": filtered,
        "source": "github_search"
    }


if __name__ == "__main__":
    # æµ‹è¯•
    from config import PAPERS_ROOT_DIR
    from utils import get_all_paper_dirs, load_paper_data
    
    print("æµ‹è¯• GitHub æœç´¢å™¨...")
    paper_dirs = get_all_paper_dirs(PAPERS_ROOT_DIR)
    
    if paper_dirs:
        test_dir = paper_dirs[0]
        print(f"\næµ‹è¯•è®ºæ–‡: {os.path.basename(test_dir)}")
        
        paper_data = load_paper_data(test_dir)
        if paper_data:
            result = search_github_implementations(paper_data, max_results=5, use_llm=True)
            print(f"\nç»“æœ: {result}")
