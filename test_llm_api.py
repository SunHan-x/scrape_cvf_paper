#!/usr/bin/env python
"""
æµ‹è¯• LLM API è¿æ¥å’ŒåŠŸèƒ½
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'LLM_repo_valid'))

import requests
import json
from LLM_repo_valid.config import GEMINI_API_KEY, GEMINI_API_URL, GEMINI_MODEL


def test_api_basic():
    """æµ‹è¯•åŸºæœ¬çš„ API è¿æ¥"""
    print("=" * 80)
    print("æµ‹è¯• 1: åŸºæœ¬ API è¿æ¥")
    print("=" * 80)
    print(f"API URL: {GEMINI_API_URL}")
    print(f"Model: {GEMINI_MODEL}")
    print(f"API Key: {GEMINI_API_KEY[:20]}..." if len(GEMINI_API_KEY) > 20 else "æœªé…ç½®")
    print()
    
    if GEMINI_API_KEY == "your_gemini_api_key_here":
        print("âŒ é”™è¯¯: API Key æœªé…ç½®!")
        print("è¯·ç¼–è¾‘ LLM_repo_valid/config.py å¹¶è®¾ç½® GEMINI_API_KEY")
        return False
    
    headers = {
        "Authorization": f"Bearer {GEMINI_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": GEMINI_MODEL,
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Say 'Hello, World!' in one word."}
        ],
        "temperature": 0.1,
        "max_tokens": 10
    }
    
    try:
        print("å‘é€è¯·æ±‚...")
        response = requests.post(
            GEMINI_API_URL,
            headers=headers,
            json=payload,
            timeout=30
        )
        
        print(f"çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”å¤´: {dict(response.headers)}")
        print()
        
        if response.status_code != 200:
            print(f"âŒ é”™è¯¯: HTTP {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text[:500]}")
            return False
        
        result = response.json()
        print("âœ… API è¿æ¥æˆåŠŸ!")
        print(f"å“åº”: {json.dumps(result, indent=2, ensure_ascii=False)}")
        return True
        
    except requests.exceptions.Timeout:
        print("âŒ é”™è¯¯: è¯·æ±‚è¶…æ—¶")
        return False
    except requests.exceptions.RequestException as e:
        print(f"âŒ é”™è¯¯: è¯·æ±‚å¤±è´¥ - {e}")
        return False
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: JSON è§£æå¤±è´¥ - {e}")
        print(f"åŸå§‹å“åº”: {response.text[:500]}")
        return False


def test_llm_client():
    """æµ‹è¯•å°è£…çš„ LLM å®¢æˆ·ç«¯"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 2: LLM å®¢æˆ·ç«¯å°è£…")
    print("=" * 80)
    
    try:
        from LLM_repo_valid.llm_client import llm_client
        
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Please say 'Hello' in one word."}
        ]
        
        print("è°ƒç”¨ llm_client.call()...")
        response = llm_client.call(messages)
        
        if response:
            print(f"âœ… æˆåŠŸ!")
            print(f"å“åº”: {response}")
            return True
        else:
            print("âŒ å¤±è´¥: æ²¡æœ‰å“åº”")
            return False
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_json_mode():
    """æµ‹è¯• JSON æ¨¡å¼"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 3: JSON æ¨¡å¼")
    print("=" * 80)
    
    try:
        from LLM_repo_valid.llm_client import llm_client
        
        messages = [
            {"role": "system", "content": "You are a helpful assistant. Reply in JSON format."},
            {"role": "user", "content": 'Say "Hello" in JSON format with a key "message".'}
        ]
        
        print("è°ƒç”¨ llm_client.call_json()...")
        response = llm_client.call_json(messages)
        
        if response:
            print(f"âœ… æˆåŠŸ!")
            print(f"å“åº”: {json.dumps(response, indent=2, ensure_ascii=False)}")
            
            if isinstance(response, dict) and "message" in response:
                print("âœ… JSON æ ¼å¼æ­£ç¡®!")
                return True
            else:
                print("âš ï¸  è­¦å‘Š: JSON æ ¼å¼å¯èƒ½ä¸ç¬¦åˆé¢„æœŸ")
                return True
        else:
            print("âŒ å¤±è´¥: æ²¡æœ‰å“åº”")
            return False
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_paper_selection():
    """æµ‹è¯•è®ºæ–‡åœºæ™¯ï¼šä»å¤šä¸ªå€™é€‰ä¸­é€‰æ‹©"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• 4: å®é™…åº”ç”¨åœºæ™¯ - é€‰æ‹©ä»£ç ä»“åº“")
    print("=" * 80)
    
    try:
        from LLM_repo_valid.llm_client import llm_client
        
        messages = [
            {
                "role": "system",
                "content": "You are a tool that picks the most likely official code repository for a paper. Reply in JSON format."
            },
            {
                "role": "user",
                "content": """Paper title: "3D Gaussian Splatting for Real-Time Radiance Field Rendering"
Venue: CVPR 2024

Found URLs in PDF:
1. https://github.com/graphdeco-inria/gaussian-splatting
2. https://github.com/some-other-user/3d-gaussian
3. https://arxiv.org/abs/2308.04079

From these URLs, which one is MOST likely the official implementation of the paper?
Reply in JSON format:
{
  "selected_url": "<url or null>",
  "reason": "brief explanation"
}"""
            }
        ]
        
        print("è°ƒç”¨ LLM é€‰æ‹©å®˜æ–¹ä»“åº“...")
        response = llm_client.call_json(messages)
        
        if response:
            print(f"âœ… æˆåŠŸ!")
            print(f"å“åº”: {json.dumps(response, indent=2, ensure_ascii=False)}")
            
            if "selected_url" in response and "reason" in response:
                print("âœ… å“åº”æ ¼å¼æ­£ç¡®!")
                print(f"é€‰æ‹©çš„ URL: {response['selected_url']}")
                print(f"ç†ç”±: {response['reason']}")
                return True
            else:
                print("âš ï¸  è­¦å‘Š: å“åº”æ ¼å¼å¯èƒ½ä¸ç¬¦åˆé¢„æœŸ")
                return False
        else:
            print("âŒ å¤±è´¥: æ²¡æœ‰å“åº”")
            return False
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("ğŸ§ª LLM API åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    print()
    
    results = []
    
    # æµ‹è¯• 1: åŸºæœ¬è¿æ¥
    results.append(("åŸºæœ¬ API è¿æ¥", test_api_basic()))
    
    # æµ‹è¯• 2: å®¢æˆ·ç«¯å°è£…
    if results[0][1]:  # å¦‚æœåŸºæœ¬è¿æ¥æˆåŠŸ
        results.append(("LLM å®¢æˆ·ç«¯å°è£…", test_llm_client()))
        results.append(("JSON æ¨¡å¼", test_json_mode()))
        results.append(("å®é™…åº”ç”¨åœºæ™¯", test_paper_selection()))
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    
    for name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{status} - {name}")
    
    total = len(results)
    passed = sum(1 for _, success in results if success)
    
    print()
    print(f"æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! LLM API åŠŸèƒ½æ­£å¸¸")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–ç½‘ç»œè¿æ¥")
        return 1


if __name__ == "__main__":
    sys.exit(main())
