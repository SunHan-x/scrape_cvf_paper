#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆç©ºå•å…ƒæ ¼ä¿®å¤è„šæœ¬
- å¡«å……ç©ºçš„Abstractã€PDF_URL
- æ£€æŸ¥CVPR_pdfæ–‡ä»¶å¤¹ä¸­æ˜¯å¦å·²æœ‰PDFï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸‹è½½
- å¡«å……PDF_Pathå­—æ®µ
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from urllib.parse import urljoin
import argparse
import os
import re

BASE_URL = "https://openaccess.thecvf.com/"
PDF_DIR = "CVPR_pdf"

def get_paper_details(paper_url):
    """
    ä»è®ºæ–‡è¯¦æƒ…é¡µè·å–æ‘˜è¦å’ŒPDF URL
    """
    try:
        response = requests.get(paper_url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # æå–æ‘˜è¦
        abstract_div = soup.find('div', id='abstract')
        abstract = abstract_div.get_text(strip=True) if abstract_div else ""
        
        # ç§»é™¤å‰å¯¼çš„ "Abstract" æ ‡ç­¾
        if abstract.startswith("Abstract"):
            abstract = abstract[len("Abstract"):].strip()
        
        # æå–PDF URL
        pdf_url = ""
        pdf_link = soup.find('a', href=lambda x: x and x.endswith('.pdf'))
        if pdf_link:
            pdf_url = urljoin(BASE_URL, pdf_link['href'])
        
        return abstract, pdf_url

    except Exception as e:
        print(f"    âŒ é”™è¯¯: {e}")
        return None, None

def sanitize_filename(filename):
    """
    æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦
    """
    # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*]', '', filename)
    # é™åˆ¶é•¿åº¦
    if len(filename) > 200:
        filename = filename[:200]
    return filename.strip()

def find_existing_pdf(pdf_dir, title, year):
    """
    åœ¨PDFç›®å½•ä¸­æŸ¥æ‰¾å·²å­˜åœ¨çš„PDFæ–‡ä»¶
    """
    if not os.path.exists(pdf_dir):
        return None
    
    # å°è¯•å¤šç§å¯èƒ½çš„æ–‡ä»¶åæ ¼å¼
    possible_names = [
        f"CVPR_{year}_{title}.pdf",
        f"CVPR_{year}_{sanitize_filename(title)}.pdf",
    ]
    
    for filename in possible_names:
        filepath = os.path.join(pdf_dir, filename)
        if os.path.exists(filepath):
            return filepath
    
    # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
    sanitized_title = sanitize_filename(title).lower()
    for filename in os.listdir(pdf_dir):
        if filename.startswith(f"CVPR_{year}_") and filename.endswith(".pdf"):
            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åœ¨æ–‡ä»¶åä¸­
            file_title = filename[len(f"CVPR_{year}_"):-4].lower()
            if sanitized_title in file_title or file_title in sanitized_title:
                return os.path.join(pdf_dir, filename)
    
    return None

def download_pdf(pdf_url, pdf_dir, title, year):
    """
    ä¸‹è½½PDFæ–‡ä»¶
    """
    try:
        os.makedirs(pdf_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        safe_title = sanitize_filename(title)
        filename = f"CVPR_{year}_{safe_title}.pdf"
        filepath = os.path.join(pdf_dir, filename)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
        if os.path.exists(filepath):
            print(f"    âœ“ PDFå·²å­˜åœ¨")
            return filepath
        
        # ä¸‹è½½PDF
        print(f"    â¬‡ï¸  ä¸‹è½½PDF...")
        response = requests.get(pdf_url, timeout=30, stream=True)
        response.raise_for_status()
        
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"    âœ… PDFä¸‹è½½æˆåŠŸ")
        return filepath
        
    except Exception as e:
        print(f"    âŒ PDFä¸‹è½½å¤±è´¥: {e}")
        return None

def fix_empty_cells(input_file, output_file, pdf_dir, start_year=2013, end_year=2025, 
                   dry_run=False, download_pdfs=True):
    """
    ä¿®å¤Excelæ–‡ä»¶ä¸­çš„ç©ºå•å…ƒæ ¼
    
    Args:
        input_file: è¾“å…¥Excelæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„
        pdf_dir: PDFæ–‡ä»¶å¤¹è·¯å¾„
        start_year: å¼€å§‹å¹´ä»½
        end_year: ç»“æŸå¹´ä»½
        dry_run: å¦‚æœä¸ºTrueï¼Œåªæ£€æŸ¥ä¸ä¿®æ”¹
        download_pdfs: æ˜¯å¦ä¸‹è½½ç¼ºå¤±çš„PDF
    """
    print("=" * 80)
    print(f"å¢å¼ºç‰ˆç©ºå•å…ƒæ ¼ä¿®å¤è„šæœ¬ - {start_year}å¹´è‡³{end_year}å¹´")
    print("=" * 80)
    print()
    
    if dry_run:
        print("âš ï¸  DRY RUN æ¨¡å¼ - åªæ£€æŸ¥ï¼Œä¸ä¿®æ”¹æ–‡ä»¶")
        print()
    
    # è¯»å–æ‰€æœ‰sheet
    df_dict = pd.read_excel(input_file, sheet_name=None)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'abstract_fixed': 0,
        'abstract_failed': 0,
        'pdf_url_fixed': 0,
        'pdf_url_failed': 0,
        'pdf_path_found': 0,
        'pdf_path_downloaded': 0,
        'pdf_path_failed': 0,
    }
    
    sheets_modified = {}
    
    for sheet_name, df in df_dict.items():
        year = int(sheet_name.split('_')[1])
        
        # åªå¤„ç†æŒ‡å®šå¹´ä»½èŒƒå›´
        if year < start_year or year > end_year:
            sheets_modified[sheet_name] = df
            continue
        
        print(f"\n{'='*70}")
        print(f"ğŸ“… å¤„ç† {sheet_name} (å…± {len(df)} ç¯‡è®ºæ–‡)")
        print(f"{'='*70}")
        
        # æ£€æŸ¥å„ç§ç©ºå€¼
        empty_abstract_mask = df['Abstract'].isna() | (df['Abstract'].astype(str).str.strip() == '') | (df['Abstract'].astype(str) == 'nan')
        empty_pdf_url_mask = df['PDF_URL'].isna() | (df['PDF_URL'].astype(str).str.strip() == '') | (df['PDF_URL'].astype(str) == 'nan')
        empty_pdf_path_mask = df['PDF_Path'].isna() | (df['PDF_Path'].astype(str).str.strip() == '') | (df['PDF_Path'].astype(str) == 'nan')
        
        empty_abstract_count = empty_abstract_mask.sum()
        empty_pdf_url_count = empty_pdf_url_mask.sum()
        empty_pdf_path_count = empty_pdf_path_mask.sum()
        
        print(f"  ç©ºAbstract: {empty_abstract_count}")
        print(f"  ç©ºPDF_URL: {empty_pdf_url_count}")
        print(f"  ç©ºPDF_Path: {empty_pdf_path_count}")
        
        if empty_abstract_count == 0 and empty_pdf_url_count == 0 and empty_pdf_path_count == 0:
            print(f"  âœ… æ— éœ€ä¿®å¤")
            sheets_modified[sheet_name] = df
            continue
        
        # åˆ›å»ºå‰¯æœ¬ä»¥ä¾¿ä¿®æ”¹
        df_fixed = df.copy()
        
        # ç¡®ä¿åˆ—çš„æ•°æ®ç±»å‹æ­£ç¡®,é¿å…pandasè­¦å‘Š
        if df_fixed['PDF_URL'].dtype != 'object':
            df_fixed['PDF_URL'] = df_fixed['PDF_URL'].astype('object')
        if df_fixed['PDF_Path'].dtype != 'object':
            df_fixed['PDF_Path'] = df_fixed['PDF_Path'].astype('object')
        
        # æ‰¾å‡ºéœ€è¦ä¿®å¤çš„è¡Œ
        needs_fix_mask = empty_abstract_mask | empty_pdf_url_mask | empty_pdf_path_mask
        needs_fix_indices = df[needs_fix_mask].index.tolist()
        
        print(f"  éœ€è¦å¤„ç† {len(needs_fix_indices)} è¡Œ")
        print()
        
        for idx, row_idx in enumerate(needs_fix_indices, 1):
            row = df.loc[row_idx]
            title = row['Title']
            url = row['URL']
            
            print(f"  [{idx}/{len(needs_fix_indices)}] å¤„ç†: {title[:50]}...")
            
            if dry_run:
                print(f"    [DRY RUN] å°†è®¿é—®: {url}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä»ç½‘ç«™è·å–ä¿¡æ¯
            need_web_data = empty_abstract_mask[row_idx] or empty_pdf_url_mask[row_idx]
            
            abstract = None
            pdf_url = None
            
            if need_web_data:
                # ä»ç½‘ç«™è·å–æ‘˜è¦å’ŒPDF URL
                abstract, pdf_url = get_paper_details(url)
                
                # æ›´æ–°Abstract
                if empty_abstract_mask[row_idx]:
                    if abstract:
                        df_fixed.loc[row_idx, 'Abstract'] = abstract
                        stats['abstract_fixed'] += 1
                        print(f"    âœ… Abstractå·²æ›´æ–° ({len(abstract)} å­—ç¬¦)")
                    else:
                        stats['abstract_failed'] += 1
                        print(f"    âš ï¸  Abstractè·å–å¤±è´¥")
                
                # æ›´æ–°PDF_URL
                if empty_pdf_url_mask[row_idx]:
                    if pdf_url:
                        df_fixed.loc[row_idx, 'PDF_URL'] = pdf_url
                        stats['pdf_url_fixed'] += 1
                        print(f"    âœ… PDF_URLå·²æ›´æ–°")
                    else:
                        stats['pdf_url_failed'] += 1
                        print(f"    âš ï¸  PDF_URLè·å–å¤±è´¥")
                
                time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
            else:
                # å¦‚æœä¸éœ€è¦ä»ç½‘ç«™è·å–ï¼Œä½¿ç”¨ç°æœ‰çš„PDF_URL
                pdf_url = row['PDF_URL'] if not empty_pdf_url_mask[row_idx] else None
            
            # å¤„ç†PDF_Path
            if empty_pdf_path_mask[row_idx]:
                # å…ˆæŸ¥æ‰¾æ˜¯å¦å·²æœ‰PDFæ–‡ä»¶
                existing_pdf = find_existing_pdf(pdf_dir, title, year)
                
                if existing_pdf:
                    df_fixed.loc[row_idx, 'PDF_Path'] = existing_pdf
                    stats['pdf_path_found'] += 1
                    print(f"    âœ… æ‰¾åˆ°å·²å­˜åœ¨çš„PDF")
                elif download_pdfs and pdf_url:
                    # ä¸‹è½½PDF
                    downloaded_path = download_pdf(pdf_url, pdf_dir, title, year)
                    if downloaded_path:
                        df_fixed.loc[row_idx, 'PDF_Path'] = downloaded_path
                        stats['pdf_path_downloaded'] += 1
                    else:
                        stats['pdf_path_failed'] += 1
                else:
                    stats['pdf_path_failed'] += 1
                    if not pdf_url:
                        print(f"    âš ï¸  æ— PDF_URLï¼Œæ— æ³•ä¸‹è½½")
                    elif not download_pdfs:
                        print(f"    âš ï¸  æœªå¯ç”¨PDFä¸‹è½½")
        
        sheets_modified[sheet_name] = df_fixed
    
    # ä¿å­˜ç»“æœ
    if not dry_run:
        print("\n" + "=" * 80)
        print("ğŸ’¾ ä¿å­˜ä¿®å¤åçš„æ–‡ä»¶...")
        print("=" * 80)
        
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            for sheet_name, df in sheets_modified.items():
                df.to_excel(writer, sheet_name=sheet_name, index=False)
        
        print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("ä¿®å¤ç»Ÿè®¡")
    print("=" * 80)
    print(f"Abstract - æˆåŠŸ: {stats['abstract_fixed']}, å¤±è´¥: {stats['abstract_failed']}")
    print(f"PDF_URL - æˆåŠŸ: {stats['pdf_url_fixed']}, å¤±è´¥: {stats['pdf_url_failed']}")
    print(f"PDF_Path - æ‰¾åˆ°å·²å­˜åœ¨: {stats['pdf_path_found']}, æ–°ä¸‹è½½: {stats['pdf_path_downloaded']}, å¤±è´¥: {stats['pdf_path_failed']}")
    print(f"æ€»æˆåŠŸ: {stats['abstract_fixed'] + stats['pdf_url_fixed'] + stats['pdf_path_found'] + stats['pdf_path_downloaded']}")
    print(f"æ€»å¤±è´¥: {stats['abstract_failed'] + stats['pdf_url_failed'] + stats['pdf_path_failed']}")
    print()

def parse_arguments():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(
        description='ä¿®å¤CVPR Excelæ–‡ä»¶ä¸­çš„ç©ºå•å…ƒæ ¼ï¼ŒåŒ…æ‹¬ä¸‹è½½PDF',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å…ˆè¿è¡Œdry runæ£€æŸ¥
  python fix_all_empty_cells.py --dry-run
  
  # ä¿®å¤æ‰€æœ‰å¹´ä»½ï¼ˆ2013-2025ï¼‰
  python fix_all_empty_cells.py
  
  # åªä¿®å¤ç‰¹å®šå¹´ä»½
  python fix_all_empty_cells.py --start-year 2020 --end-year 2025
  
  # ä¸ä¸‹è½½PDFï¼Œåªå¡«å……å…¶ä»–ä¿¡æ¯
  python fix_all_empty_cells.py --no-download-pdf
        """
    )
    
    parser.add_argument(
        '--input', '-i',
        type=str,
        default='CVPR_xlsx/CVPR_2013_2025.xlsx',
        help='è¾“å…¥Excelæ–‡ä»¶è·¯å¾„ (é»˜è®¤: CVPR_xlsx/CVPR_2013_2025.xlsx)'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='CVPR_xlsx/CVPR_2013_2025_fixed.xlsx',
        help='è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„ (é»˜è®¤: CVPR_xlsx/CVPR_2013_2025_fixed.xlsx)'
    )
    
    parser.add_argument(
        '--pdf-dir', '-p',
        type=str,
        default='CVPR_pdf',
        help='PDFæ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤: CVPR_pdf)'
    )
    
    parser.add_argument(
        '--start-year', '-s',
        type=int,
        default=2013,
        help='å¼€å§‹å¹´ä»½ (é»˜è®¤: 2013)'
    )
    
    parser.add_argument(
        '--end-year', '-e',
        type=int,
        default=2025,
        help='ç»“æŸå¹´ä»½ (é»˜è®¤: 2025)'
    )
    
    parser.add_argument(
        '--no-download-pdf',
        action='store_true',
        help='ä¸ä¸‹è½½PDFæ–‡ä»¶'
    )
    
    parser.add_argument(
        '--dry-run', '-d',
        action='store_true',
        help='åªæ£€æŸ¥ä¸ä¿®æ”¹ (dry runæ¨¡å¼)'
    )
    
    return parser.parse_args()

def main():
    args = parse_arguments()
    
    print(f"é…ç½®:")
    print(f"  è¾“å…¥æ–‡ä»¶: {args.input}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"  PDFç›®å½•: {args.pdf_dir}")
    print(f"  å¹´ä»½èŒƒå›´: {args.start_year} - {args.end_year}")
    print(f"  ä¸‹è½½PDF: {not args.no_download_pdf}")
    print(f"  Dry Run: {args.dry_run}")
    print()
    
    fix_empty_cells(
        input_file=args.input,
        output_file=args.output,
        pdf_dir=args.pdf_dir,
        start_year=args.start_year,
        end_year=args.end_year,
        dry_run=args.dry_run,
        download_pdfs=not args.no_download_pdf
    )

if __name__ == "__main__":
    main()
