import requests
from bs4 import BeautifulSoup
import pandas as pd
import random
import time
from urllib.parse import urljoin

BASE_URL = "https://openaccess.thecvf.com/"

def get_paper_count_from_website(year):
    """
    ä»ç½‘ç«™è·å–æŒ‡å®šå¹´ä»½çš„è®ºæ–‡æ€»æ•°
    """
    # å…ˆå°è¯•ä½¿ç”¨ day=all
    url_all = f"{BASE_URL}CVPR{year}?day=all"
    try:
        response = requests.get(url_all, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ ‡é¢˜
        dt_elements = soup.find_all('dt', class_='ptitle')
        
        if len(dt_elements) > 0:
            # day=all æˆåŠŸ
            return len(dt_elements)
        
        # day=all è¿”å›0,å°è¯•è·å–å„ä¸ªdayçš„è®ºæ–‡æ•°
        print(f"    day=all æ— æ•ˆ,å°è¯•é€å¤©è·å–...")
        return get_paper_count_by_days(year)
        
    except Exception as e:
        print(f"  âŒ è·å– {year} å¹´è®ºæ–‡æ•°å¤±è´¥: {e}")
        return None

def get_paper_count_by_days(year):
    """
    é€šè¿‡é€å¤©è·å–æ¥ç»Ÿè®¡è®ºæ–‡æ€»æ•°(å½“day=allä¸å¯ç”¨æ—¶)
    """
    main_url = f"{BASE_URL}CVPR{year}"
    
    try:
        response = requests.get(main_url, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰dayé“¾æ¥
        day_links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            if '?day=' in href and href not in day_links:
                day_links.append(href)
        
        if not day_links:
            print(f"    âš ï¸  æœªæ‰¾åˆ°dayé“¾æ¥")
            return None
        
        total_papers = 0
        for link in day_links:
            full_url = urljoin(BASE_URL, link)
            try:
                response = requests.get(full_url, timeout=20)
                response.raise_for_status()
                day_soup = BeautifulSoup(response.text, 'html.parser')
                dt_elements = day_soup.find_all('dt', class_='ptitle')
                count = len(dt_elements)
                total_papers += count
                
                # æå–dayåç§°
                day_name = link.split('day=')[1].split('&')[0] if '&' in link.split('day=')[1] else link.split('day=')[1]
                print(f"      {day_name}: {count} ç¯‡")
                
                time.sleep(0.3)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                print(f"    âš ï¸  è·å– {link} å¤±è´¥: {e}")
        
        return total_papers
        
    except Exception as e:
        print(f"    âŒ è·å–ä¸»é¡µé¢å¤±è´¥: {e}")
        return None

def verify_paper_details(paper_url, expected_title, expected_abstract):
    """
    éªŒè¯å•ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
    """
    try:
        response = requests.get(paper_url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # æå–æ ‡é¢˜
        title_elem = soup.find('div', id='papertitle')
        actual_title = title_elem.get_text(strip=True) if title_elem else ""
        
        # æå–æ‘˜è¦
        abstract_div = soup.find('div', id='abstract')
        actual_abstract = abstract_div.get_text(strip=True) if abstract_div else ""
        
        # ç§»é™¤ "Abstract" æ ‡ç­¾
        if actual_abstract.startswith("Abstract"):
            actual_abstract = actual_abstract[len("Abstract"):].strip()
        
        # éªŒè¯æ ‡é¢˜
        title_match = actual_title.strip() == expected_title.strip()
        
        # éªŒè¯æ‘˜è¦ï¼ˆå…è®¸ä¸€å®šçš„æ ¼å¼å·®å¼‚ï¼‰
        abstract_match = actual_abstract.strip() == expected_abstract.strip()
        
        return {
            'title_match': title_match,
            'abstract_match': abstract_match,
            'actual_title': actual_title,
            'actual_abstract': actual_abstract[:100] + '...' if len(actual_abstract) > 100 else actual_abstract
        }
    except Exception as e:
        return {
            'title_match': False,
            'abstract_match': False,
            'error': str(e)
        }

def main():
    print("=" * 80)
    print("CVPR æ•°æ®éªŒè¯æŠ¥å‘Š")
    print("=" * 80)
    print()
    
    # è¯»å– Excel æ–‡ä»¶
    excel_path = 'CVPR_xlsx/CVPR_2013_2025.xlsx'
    print(f"ğŸ“‚ è¯»å–æ–‡ä»¶: {excel_path}")
    
    try:
        df_dict = pd.read_excel(excel_path, sheet_name=None)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    print(f"âœ… æˆåŠŸè¯»å– {len(df_dict)} ä¸ªsheet")
    print()
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šéªŒè¯è®ºæ–‡æ•°é‡
    print("=" * 80)
    print("ç¬¬ä¸€éƒ¨åˆ†ï¼šéªŒè¯å„å¹´ä»½è®ºæ–‡æ•°é‡")
    print("=" * 80)
    print()
    
    count_results = []
    
    for sheet_name, df in df_dict.items():
        year = sheet_name.split('_')[1]
        excel_count = len(df)
        
        print(f"ğŸ“… éªŒè¯ {year} å¹´...")
        print(f"  Excelä¸­çš„è®ºæ–‡æ•°: {excel_count}")
        
        website_count = get_paper_count_from_website(year)
        
        if website_count is not None:
            print(f"  ç½‘ç«™ä¸Šçš„è®ºæ–‡æ•°: {website_count}")
            match = excel_count == website_count
            diff = excel_count - website_count
            
            if match:
                print(f"  âœ… æ•°é‡ä¸€è‡´")
            else:
                print(f"  âš ï¸  æ•°é‡ä¸ä¸€è‡´ (å·®å¼‚: {diff:+d})")
            
            count_results.append({
                'å¹´ä»½': year,
                'Excelæ•°é‡': excel_count,
                'ç½‘ç«™æ•°é‡': website_count,
                'å·®å¼‚': diff,
                'æ˜¯å¦ä¸€è‡´': 'âœ…' if match else 'âŒ'
            })
        else:
            count_results.append({
                'å¹´ä»½': year,
                'Excelæ•°é‡': excel_count,
                'ç½‘ç«™æ•°é‡': 'N/A',
                'å·®å¼‚': 'N/A',
                'æ˜¯å¦ä¸€è‡´': 'âŒ'
            })
        
        print()
        time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šæŠ½æ ·éªŒè¯è®ºæ–‡ä¿¡æ¯
    print("=" * 80)
    print("ç¬¬äºŒéƒ¨åˆ†ï¼šæŠ½æ ·éªŒè¯è®ºæ–‡è¯¦ç»†ä¿¡æ¯ (æ¯å¹´20ç¯‡)")
    print("=" * 80)
    print()
    
    sample_results = []
    
    for sheet_name, df in df_dict.items():
        year = sheet_name.split('_')[1]
        print(f"ğŸ“… éªŒè¯ {year} å¹´çš„è®ºæ–‡è¯¦æƒ…...")
        
        # éšæœºæŠ½å–20ç¯‡ï¼ˆå¦‚æœè®ºæ–‡æ•°ä¸è¶³20ç¯‡ï¼Œåˆ™å…¨éƒ¨æŠ½å–ï¼‰
        sample_size = min(20, len(df))
        sample_indices = random.sample(range(len(df)), sample_size)
        
        year_correct = 0
        year_total = 0
        
        for idx in sample_indices:
            paper = df.iloc[idx]
            year_total += 1
            
            print(f"  [{year_total}/{sample_size}] éªŒè¯: {paper['Title'][:50]}...")
            
            result = verify_paper_details(
                paper['URL'],
                paper['Title'],
                paper['Abstract']
            )
            
            if 'error' in result:
                print(f"    âŒ éªŒè¯å¤±è´¥: {result['error']}")
                sample_results.append({
                    'å¹´ä»½': year,
                    'è®ºæ–‡æ ‡é¢˜': paper['Title'][:50] + '...',
                    'æ ‡é¢˜åŒ¹é…': 'âŒ',
                    'æ‘˜è¦åŒ¹é…': 'âŒ',
                    'é”™è¯¯': result['error']
                })
            else:
                title_status = 'âœ…' if result['title_match'] else 'âŒ'
                abstract_status = 'âœ…' if result['abstract_match'] else 'âŒ'
                
                print(f"    æ ‡é¢˜: {title_status}")
                print(f"    æ‘˜è¦: {abstract_status}")
                
                if result['title_match'] and result['abstract_match']:
                    year_correct += 1
                    print(f"    âœ… ä¿¡æ¯æ­£ç¡®")
                else:
                    print(f"    âš ï¸  ä¿¡æ¯ä¸åŒ¹é…")
                    if not result['title_match']:
                        print(f"      æœŸæœ›æ ‡é¢˜: {paper['Title'][:50]}")
                        print(f"      å®é™…æ ‡é¢˜: {result['actual_title'][:50]}")
                
                sample_results.append({
                    'å¹´ä»½': year,
                    'è®ºæ–‡æ ‡é¢˜': paper['Title'][:50] + '...',
                    'æ ‡é¢˜åŒ¹é…': title_status,
                    'æ‘˜è¦åŒ¹é…': abstract_status,
                    'é”™è¯¯': ''
                })
            
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        accuracy = (year_correct / year_total * 100) if year_total > 0 else 0
        print(f"  ğŸ“Š {year} å¹´å‡†ç¡®ç‡: {year_correct}/{year_total} ({accuracy:.1f}%)")
        print()
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("=" * 80)
    print("éªŒè¯æ±‡æ€»æŠ¥å‘Š")
    print("=" * 80)
    print()
    
    print("ğŸ“Š è®ºæ–‡æ•°é‡å¯¹æ¯”:")
    count_df = pd.DataFrame(count_results)
    print(count_df.to_string(index=False))
    print()
    
    # ç»Ÿè®¡æ•°é‡ä¸€è‡´æ€§
    match_count = sum(1 for r in count_results if r['æ˜¯å¦ä¸€è‡´'] == 'âœ…')
    total_years = len(count_results)
    print(f"æ•°é‡ä¸€è‡´çš„å¹´ä»½: {match_count}/{total_years} ({match_count/total_years*100:.1f}%)")
    print()
    
    print("ğŸ“‹ æŠ½æ ·éªŒè¯ç»Ÿè®¡:")
    sample_df = pd.DataFrame(sample_results)
    
    # ç»Ÿè®¡æ¯å¹´çš„å‡†ç¡®ç‡
    for year in sorted(set(r['å¹´ä»½'] for r in sample_results)):
        year_samples = [r for r in sample_results if r['å¹´ä»½'] == year]
        correct = sum(1 for r in year_samples if r['æ ‡é¢˜åŒ¹é…'] == 'âœ…' and r['æ‘˜è¦åŒ¹é…'] == 'âœ…')
        total = len(year_samples)
        print(f"  {year}: {correct}/{total} æ­£ç¡® ({correct/total*100:.1f}%)")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°Excel
    report_path = 'CVPR_xlsx/verification_report.xlsx'
    with pd.ExcelWriter(report_path, engine='openpyxl') as writer:
        count_df.to_excel(writer, sheet_name='è®ºæ–‡æ•°é‡å¯¹æ¯”', index=False)
        sample_df.to_excel(writer, sheet_name='æŠ½æ ·éªŒè¯è¯¦æƒ…', index=False)
    
    print()
    print(f"âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

if __name__ == "__main__":
    main()
